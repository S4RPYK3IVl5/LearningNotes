1. Hadoop: 
 1.1. HDFS - распределенная файловая система, устойчивая к ошибкам.
  Присутствует тройная репликация.
  Каждый файл разбивается на блоки фиксированного размера, которые репликуются и распределяются между разными нодами.
  HDFS более разработана для обработки батч
  Так же поддерживает огромные данные.
  Выгоднее всего делать расчеты на местах где и расположены данные.
  NameNode - отвечает за аллокейт ресурсов в hdfs (для безопасности делается его копия) и отслеживание жизни нейм нодов, если в течении 10 минут не приходит биение сердца от DataNode, то такая нода считается погибшей. Если он отвалится, то вся hdfs прийдет в негодность.
  DataNode - отвечает за конкретный узел в кластере, его состояние и возможности. Постоянно посылает свое состояние NameNode, а иначе считаться будет погибшим. Если отвалится, то NameNode передаст его ресурсы между другими нодами.
  Размер блока и replication factor рассматривается относительно файла.
  rack - подиножество node.
  Реплика всегда насчитывается таким образом что бы не был сильной задержки.
  Увеличить размеры кластера не всегда бывает отличным решением для улучшения работы HDFS, много разных задача требуют разное (CPU speed or more threads). Оптимальный объем данных для одного компьютера 64gb.
  Для hadoop важна очень пропускная способность, чтобы была быстрая скорость при передаче данных между кластерами.

 1.2. MapReduce v1 - парадигма программирования версии 1 в Hadoop была построена на основе работы Job Tracker and Task Tracker.
  Батч ориентированный.
  Job Tracker - Работает на отдельном ноде от DataNode. Получает запрос на MapReduce работу от клиента. Говорит NameNode определить локацию нужных для работы данных. Находит самый подходящий Task Tracker основываясь на доступности ресурсов и возможности выполнять таску для выполнения работы клиента. Мониторит жизненный цикл Task Tracker. Очень критичен для все экосистемы Hadoop так как один на все кластеры. Если отвалится, невозможно будет принимать работу и существующие работы потеряются, хотя работа продолжится.
  Task Tracker - запускается на всех DataNode. Отвечает за выполнение MapReduce задачи. Постоянно делится своим состоянием с Job Tracker. Падение не фатально, так как Job Tracker если что переназначит выполнение задачи. 
  Map -> (Combiner) -> Shuffel, Sorting -> Reduce

 1.3. MapReduce v2 (YARN) - изменения в hadoop которые позволили нам делать разные работы, не только Map Reduce.
  ResourceManager - высший орган руководства, который заправляет всеми ресурсами между приложениями. Имеет два важных концепта: Scheduler and ApplicationsManager. 
   1.3.1 Scheduler - это простой планировщик задач, который назначают нужные ресурсы определенным задачам, ставит работы в очереди по признакам, формирует контейнеры для захвата нужных ресурсов.
  NodeManager - отвечает за ресурсы своего компьютера, следит за контейнерами и отправляет данные ResourceManager/Scheduler.
  ApplicationMaster - сервис, который пишут клиенты, отвечающий за согласование ресурсов с 
ResourceManager и работу с NodeManager(s) для выполнения и мониторинга определенного задания. Коммуницирует с ResourceManager. Связываться с контейнерами, которые назначил планировщик и мониторит их жизнь и ход выполнения работы.
  Container - единица задачи относительно какой-то работы, которая несет в себе информацию о занимаемой памяти и выполнении работы.
  ApplicationsManager - назначает контейнер для несения ApplicationMaster в нем и предоставляет тулы для перезапуска ApplicationsMaster когда он отвалился (мониторит его жизнь).

2. Apache Hive: 
 Прежде всего это warehouse.
 Имеет привычный всем SQL синтаксис для анализа и репортов на огромном количестве данных.
 Помимо всего прочего можно определять юзерские функции.
 Hive не приспособен к обработке онлайн транзакций. Не пригоден для апдейтов и запросов в реальном времени. Не может быть использован в запросах где нужна минимальная задержка 
 Все SQL запросы будут конвертироваться в map reduce.
 Поддерживает большое количество форматов данных, такие как: Avro, txt, orc, scv, rc.
 Метаданные хранятся в RDBMS (Derby)
 Позволяет совершать запросы на основе сжатых данных
 Есть такой тип данных как STRUCT and REPEATED
 C помощью Apache tez можно ускорить работу
 Так же имеются партишоны и бакеты.
 Embedded Metastore - встроенная датастор, которая вертится на той же jvm что и hive
 Local Metastore - бд которая вертится на отличном от hive процессе но локально
 Remote Metastore - заводим уделенный метастор
 ORC - Идеальный формат данных, который подходит для Hive. Есть индексы, колонки, поддерживает типы данных, специфичные для warehouse. Удобен со стримами и поддерживает очень быстрый поиск (особенно индексация + фильтр).

3. Apache Spark:
 Это быстрый механизм обработки данных в памяти.
 Позволяет обрабатывать стриминговые процессы.
 Приходит на замену MapReduce когда нужна маленькая задержка на больших данных.
 Он расширяет модель MapReduce.
 Существует множество библиотек для расширение Spark.
 Параллельное и ошибко-устойчивое выполнение Spark приложений.
 Используется для анализа, трансформирования, статистики, ML и SQL.
 Spark SQL, Spark Streaming, MLib, GraphX - популярные библиотеки (можно комбинировать).
 Spark SQL - работать с Spark через HQL and SQL (на ЯП). Создает новый SchemaRDD. SQLContext создается из SparkContext. Определить схему можно с помощью JSON, DataFrame, Reflection and etc.
 Spark Streaming - библиотека для работы с стримами. Обработка стриминговых данных приходящих маленькими батчами. DStream - последовательность RDD. Данные могут приходить с HDFS, Twitter, Kafka etc. А уходят в Dataset, HDFS, Dashboard. Window - хранит в себе объединение нескольких DStream. Window параметризуется размером(через какое время создаётсяя) и временем(как часто создается). При создании StreamingContext задается количество потоков и задается интервал обрабатываемых сообщений.
 MLib - библиотека, которая позволяет писать ML программы, поддерживает множество ML алгоритмов. Содержит в себе множество библиотек для работы с почти всеми алгоритмами ML.
 GraphX - библиотека для работы с графами. Для моделировния соц сети и языка. Помогает сконструировать граф, модифицировать, и совершать вычисления распространенные на нескольких графов.
 Главная его задача была решить проблемы которые не решал Map Reduce со своими батчами.
 Решает главную проблему с Map Reduce, с его бесконечным вводом выводом, реализацией и сериализацией, с помощью RDD.
 Главная идея RDD в том что Промежуточные результаты хранятся в оперативной памяти, а не записывается на HDFS как это сделано в MapReduce, при этом, если будет удобнее хранить промежуточные данные в памяти, Spark будет хранить их там.
 RDD - иммутабельный. Позволяет параллельный доступ к своим данным.
 RDD имеет два типа операций Action and Transformation.
 Transformation - действия над RDD, возвращающие RDD.
 Action - действия над RDD, возвращающие результат.
 3 способа получения RDD: 
  1) Распараллелить коллекцию.
  2) Ссылаться на датасет.
  3) Трансформация из существующего RDD.
 Датасетом является любой файл, который поддерживается Hdfs, Cassandra, S3 and etc.
 Набор действий, совершаемых над RDD является DAG.
 Можно настраивать тип хранения данных в RDD.
 Когда несколько раз считываются данные, гораздо легче их сразу выполнять в памяти, чем каждый раз читать (cache()). 
 Каждая Spark работа состоит из Driver(main функция пользователя) и параллельные операции на нодах.
 Когда выполняются функци на своих машинах, они используют обычно копии переменных, но есть два способа их расшарить:
  1) Broadcast - передача only read переменных во все ноды с оптимизированным способом передачи. Переменные именно кешируются, а не передаются с вызовом.
  2) Accumulator - только номерной тип данных, они могут быть записаны ото всюду, но считаны только в драйвер программе. Хорошо реализуется sum and Count. Программистами может быть расширен этот тип данных.
 DataFrame - это такой тип вида данных, который представляется в виде таблицы под управлением Pandas. Может быть назначен индекс (строка, дата или число). Данные могут быть импортированы с CSV, Dictionaries or Numpy arrays.
 SQLContext уже вшитый в Spark Context.
 DataFrame мб быть зарегистрирована как таблица Spark SQL, мб произведены SQL запросы над такой таблицей. 
 SparkConnetcon - входная точка в спарк, с помощью него создаем переменные, RDD и всячески манипулируем работой спарк.
 Spark состоит из 3 главных компонентов:
  1) Driver Program(SparkContext) - main программа spark'a тут.
  2) Cluster Manager() - Standalone, Apache Mesos, Hadoop YARN - управяет жц и назначением тасок.
  3) Worker Node - где лежит Executor (происходит выполнение тасок и хранение данных для задания)
 Driver Program шлет каждому Executor программу которую они будут выполнять.
 Каждое приложение имеет свой собственный Executor, из этого следует что не могу шарить переменные, так как разворачиваются на разных JVM процессах.
 Можно настраивать кучу пропертей в Spark и на каждой ноде.
 Combiner следует использовать только тогда когда не имеет смысла порядок переменных.
 Существует великое множество notebooks для разработки на Python, spark, Scala and etc.
 RDD по существу это коллекция элементов разделенная по кластерам.
 Поддерживает чтение с нескольких дата сторов, не только с HDFS. К примеру: local System, Google, AWS, Azure and etc., HBase, Cassandra, MongoDB. Можно так же реализовать свой кастомный инпут формат.
 inputFormat так же назначает разделение и расположение частей выполнения RDD.
 Партишонинг наследуется от родителя, и следовательно у нас есть готовая структура наследования партишонов от rdd k rdd. Структура наследование партишонов схожа на структуру linked List.
 Назначать партишоны нужно исходя из количества ядер в процессоре и избегать переменки данных между узлами.
 Filter or map не меняют партишоны.
 Однако можно в ручную изменить количество партишонов двумя способами:
  1) Repartition - увеичить число после фильтра, что приведет у увеличению параллелизма, но будет shuffle.
  2) Coalesce - уменьшить число, но без shuffle, но теряем параллелизм. Использутеся перед записью в место ранения.
 Добавляя ключи к значениям мы не изменяем его партишионинга, значения с одинаковыми ключами могут быть на разных нодах.
 Но мы можем партишонить значения по одному и тому же ключу.
  RangePartition - сначала сортирует по ключам потом по значениям.
  HashPartiton - назначаем количество партишонов, и он уверяет что значения с одним и тем же хешем ключа будут в одном партишоне.
 PartitionBy() - вызывает shuffle и ведет к понятно чему.
 Если у нас часто применяется shuffle нужно задуматься о partitionBy.
 Из-за того что у нас RDD хранят свое наследственность => мы можем в случае ошибки переделать какую-либо часть
 Job -> Stages -> Tasks
 DAG scheider анализирует Dag для определения stages and tasks.
 Stage boundaries - stage - shuffle. 
 Task должна быть сериализуема, чтобы мы могли ее пересылать от ноды к ноде.
 Transient - метка которая создает переменную на каждой таске, вместо того чтобы ее передавать по сети, такое решение подходит в случаях, когда мы владеем несериализуемым объектом.
 Speculative execution - опция, которая перезапускает сложную таску, если она занимает много времени на выполнение.
 Но при перезапуске таски нужно опасаться того, что может быть записаны еще раз выходные значения этой таски. 
 По дефолту schelduer имплементит самый обычный вид очереди FIFO, при этом выдавая все возможные ресурсы для таски. FIFO не позволяет асинхронность.
 Но можно использовать к примеру Fair Schelduer.
 К RDD можно применять набор математических статических операций (min, Max, avg and etc)
 mapPartitions - функция для применения действий к целому набору, нежели к 1 элементу.
 foreachPartitions - понятно 
 Approximate Calculations - выдает приблизительные вычисления на больших датасетах. 
 fold - accumulator функции.
 aggregate - выполняет сложные накопления, сначала по партишонам, потом комбенирует.
 countByValue - выполняет подсчет, находя сумму элементов
 Actions - блокирующие операции.
 Если наши ключи отсортированы по партишонам, а следом вызвать обычную не по ключам операцию, то Spark более не будет знать что у нас партишон по ключам и применит shuffle.
 Следует использовать aggregateByKey() вместо groupByKey() для того чтобы сгруппировать элементы перед shuffle
 Хорошо бы было хранить данные после фильтрации, обрезания.
 Сохранять RDD нужно при ветвлении.
 В памяти лучше хранить стерилизованные объекты вместо просто Java объектов.
 Можно всячески настраивать то сколько и как будут храниться в памяти объекты.
 Рекомендуется кешировать объекты когда у нас произошли тяжелые вычисления, чтобы не перекомпелировать, а просто взять из кеша.
 Если записать одну из rdd в броадкаст, а потом ее сджоинить с другой, то получится, что у нас не будет shuffle шага.
 Если не помещается объект в память, то он будет каждый раз перекомпилироваться(MEMORY_ONLY).
 RDD compiled-time type safe. 
 RDD поддерживает LAZY выполнение.
 Обязательно сначала нужно фильтровать, а зачем уменьшать
 DataSet расширение DataFrame.
 DataSet использует строго типизированные типы в отличие от DF.
 DS и DF позволяют обрабатывать структурированные и нестроктурированные типы данных.
 DS and Df Lazy
 DS во время того как планирует что-то выполнять, оптимизирует запрос.
 DS поддерживает лямбда функции, а DF нет.
 После Spark 2.0 DS and DF были унифицированы.
 DF - это просто DS[Row]
 Row - это общий тип представления строк в БД, если мы хотим типизировать DS, то нужно использовать as().
 По мимо всего прочего Spark добавил множество функция для упрощения выполнения SQL запросов. 
 Не важно на каком языке выполняется и используется DF or DS, план исполенния в Spark будет всегда одинаковый. К примеру произвольный вызов Spark функции связанной с SQL делает ее под капотом SQL подобной.
 Если явно указывать в stagе что мы хотим сделать, без UDF, Spark будет знать что мы хотим  и с оптимизирует запрос.
 Spark Schelduer достаточно умный, он позволяет нам размещать работу там, где она находится, например если файл на hdfs, то гораздо выгоднее будет выполнять работу там, чем перемещать весь файл через сеть.
 Driver Program -> Job -> Stages -> Task
 Spark имеет свой личный способ сериализации (Encoder), который позволяет нам знать что лежит в стерилизуемом объекте, и работать с ним, полностью не десериализуя его, или вообще работать на сериализованном объекте.
 Неважно откуда берутся данные, Spark транслирует их в объекты автоматически с помощью (Encoder).
 Сatalyst - оптимизатор, который оптимизирует все запросы выходящие с DS или DF.
 Catalyst позволяет нам избавится от проблем со стриминговыми данными при работе с ними.
 Изменение коснулось DataFrame, который позволил нам обработать в 2.0 работать со стриминговыми приходящими данными через ContiniousDataFrame.
 Унифициремые запросы для Batch, Streaming and Interactive запросов.
 Высокоуровневый Streaming API построен на Apache Spark SQL engine, позволяющий писать те же DataFrame запросы и сохраняет Windowing.
 Он же позволяет обрабатывать запросы к SQL в потоке, изменять запросы в рантамйме, Строить и применять ML.
 Разница в Batch и Continious записи в том что:
  1) Scan Files : Scan New Files
  2) Aggregate : Aggregate Statefull
  3) Write to SQL : Update to SQL
 RunJob() - функция, которая запускает работу на rdd. (Spark Context)
 При вызове функции выше, проходимся по всем предкам rdd и вызываем их функции.
 Когда dag построен, он не выполняется сразу, сначала происходит оптимизация:
  Конкретная часть rdd может быть закеширована.
 RDD не просто указывает на потомка, но он так же и держит некую метаинформацию:
  1) Narrow Dependency - Когда каждый партишон в родители ссылается максимум 1 партишон потомка.
  2) Shuffle (Wide) Dependency - Это когда родительский партишон может ссылаться на несколько партишонов потомка.
 RDD состоит из 4 вещей: 
  1) Partitions
  2) Dependencies
  3) Function
  4) Metadata
 Spark оптимизирует работу в том случае, если несколько RDD связаны Narrow Dependency, он связывает несколько narrow операций в одну(производит один stage из нескольких).
 Для оптимизации очень важно контролировать количество партишонов исходя из количества данных.
 Хранение стерилизованных объектов делает жизнь GB проще, но с другой стороны делает считывание дольше.
 Считается нормальным иметь executor с 64gb памяти, чтобы не перегружать JVM.
 Нужно убедиться то что дали shuffle использовать столько ресурсов сколько возможно.
 Главные проблемы при работе с Spark: 
  1) Network - страдающая пропускная способность, не позволяющая пройти большому количеству данных быстро.
  2) Stragglers - Большая задача использует не оптимально ресурсы.
 Нужно стремится к тому чтобы таски не блокировались на стадии Network and Disc.

3. Load Scenarios Data into Hadoop:
 Есть 4 вида загрузки данных:
  1) At rest - Данные уже составлены, загружаются в Hadoop as is. Для загрузки используется стандартная команда : cp, put, copyFromLocal.
  2) In motion - Данные загружаются постоянно, их нужно добавлять к существующему сорсу. Пример: соединение несколько логов из одной директории, в один файл на hdfs. 
  3) From web server - logs из веб сервера, WebSphere. Можно использовать Flume или JMX.
  4) From a data warehouse - использовать стандартную команду ДБ для экспорта данных в файл с разделителем, использовать Sqoop, через Hadoop команды или через разные модули.

4. Flume:
 Используется с помощью cmd.
 Первично используется для сбора логов и сохранения их на HDFS.
 Распределенный сервис для собирания данных и хранения их в централизованном хранилище.
 Обычно стриминговые данные.
 Используется таким образом, что читает с помощью своего агента с какого-либо сорса, а потом перенаправляет считанные данные какому-либо агенту на стороне назначения.
 Представляет собой трех-уровневый слой, первый слой на источнике данных, второй слой собирает данные, третий слой их принимает на точке назначения. 
 Все агенты соединены с сорсом и синком, по идеи один агент может всем управлять.
 Состоит из узлов, соединенных с друг другом.
 Каждый узел состоит из сорса, канала и синка. 
 Присутствует физически на одном кластере JVM.
 Клонирование и объединение уровней так же саппортится.
 Каждый агент должен быть сконфигурирован компонентами, а затем уже компоненты должны быть скнофигурированы и в конце связаны через канал. 
 Все конфигурации начинаются с имени агента => Агенты читают только конфигурации которые помечены их именем => Может быть заявлено несколько агентов в 1 файле.
 Сорсами могут быть: Avro, HTTP res, Sequence generator, JSON and etc.
 Синками могут быть: HDFS, Logger, Avro, Hbase, custom and etc.
 Каналами могут быть: Memory, JDBC, File, Custom.
 Разный синк по разному конфигурирутся.
 В зависимости от сорса, канала или синка  могут быть разные настройки.
 Interceptor позволяет менять или дропать ивенты на лету.
 На сорс может быть навешано множество интерсепторов. Они исполняются в порядке объявления. Каждый интерсептор прогоняет пришедшие данные через набор установок(можно создать свою). Например: навешивается timestamp, Host, Static, RefexpFilter (анализирует бади исходя из regex и решает что делать с ивентов), RegexpExtractor(выбирает подходящие группы из ивента и добавляет их в header).
 Каждый ивент состоит из Header and Body. Данные лежат в боди, а хедер как правило пустой.
 selector.type для сорсов, говорит о том, будет ли копироваться сообшение всем каналом, или одно на набор каналов, или настроить дефолтное поведение.
 Помогает избежать низкой пропускной способности, так как создает один коннекшн, вместо нескольких.
 Существует транзакционный обмен между Агентами, увеличивает надежность. В Sink создается объект Transaction, который лежит в Channel, если передача произошла успешно, то транзакция закрывается. В Source такой же процесс.
 Event - кусок данных, который плывет через Agent.
 Sink and Source работают асинхронно.
 Sink удаляет Event из своего Channel только после того как оно буде прочтено Channel следующего Agent'a или сохранено в конченое хранилище.
 Flume использует транзакционный подход по передачи сообщения. Transaction энкапсулируют Events.
 Client - процесс который работает на стороне с которой берутся данные для обработки Flume
 Довольно легко может оказаться, что нам не достаточно существующих Client and Source, мы можем написать своего клиента, который сможет комуницировать с существующими Source, или самому написать одно и другое так, чтобы наши данные были завернуты в Event.
 ClientSDK - библиотека для простого построения клиента.
 Можно иметь встроенного агента на стороне клиента, правда не все виды компонентов будут доступны, так как должна сохраняться легковесность агента так же присутствуют пару ограничений по поводу настроек доступных компонентов.
 Sink - главная цель - это передать данные в другие агенты или в конченое хранилище. Ассоциирован с именно одни Channel. SinkRunner ассоциирован только с одним Sink. Sink обязательно должен реализовать методы Start, stop, process.
 Source - пульт сообщения с какого-либо Sink и передает их в Channel. Так же реализует Start, stop, process. Есть два вида Source PollableSource and EventDrivenSource. В случае ошибки, будут отозваны все сообщения с каналов, а сообщения, успешно переданные в каналы, там так и останутся.
 Failover Sink Processor - поддерживает отказоустойчивость для Sink, таким образом, создает их несколько штук, и пока один из них работает, работа будет продолжаться.
 Можно размножить Channel в пределах одного Agent двумя способами:
  1) Multiplexing - событие шлются только исходя из предварительно настроенного атрибута. Маппинг на соответствие значений к каналам мб определен в конфигурационном файле.
  2) Replicating - ивент шлется всем Channels.

5. Sqoop:
 Разработан для эффективной трансформации данных между Hadoop и RDBMS.
 Имеет cmd для трансформации данных.
 Поддерживает motion загрузки из/в таблицу бд, или SQL query, или свободно запускаемые скрипты.
 Мб использован для заселения таблиц в Hive или HBase.
 Поддерживает множество конекторов для целей разных видов.
 Использует JDBC для соединения с БД.
 Использует БД для описание схемы таблицы.
 Использует MapReduce для экспорт или импорта в или из дб.
 Когда импортирует в HDFS Sqoop генерирует джава классы для инкапсуляции каждой строчки. Код к таким классам доступен для разработчика.
 Каждая строчка в бд становится отдельной записью в HDFS.
 Данные могут храниться в: txt, binary, HBase, Hive.
 Импортированные данные могут быть: строчки и колонки, можно ограничить колонки и строки, можно задать свой кастомные запрос для доступа к БД.
 По дефолту данные в HDFS сохраняются в директории с названием от таблицы.
 Импортирует данные параллельно. По дефолту использует 4 маппера. Распределяет он поровну, основываясь на значении первичного ключа.
 По дефолту данные сохраняются в виде текста, но так же можно сохранить как Avro или sequence. 
 Для экспорта таблица должна уже существовать, записи размещаются основываясь на спецификации пользователя.
 По дефолту используется insert, но можно использовать update. Так же можно вызывать хранимую процедуру на каждую вставляемую строку.
 По дефолту предполагает что при экспорте в файле будут поля разделены запятой, а записи новой линией, но это поведение можно настроить.
 ???Каждые 100 вставок по 100 строк подкрепляются транзакционно.???
 
6.Hadoop Job Orchestration:
 ETL - Extract Transform Load. Довольно бширная тема, которая включает в себя: Data Integration, Organisation and Management, Process orchestration and scheduling, Accessibility.
 Для того, чтобы ETL был расширяемым, следует разделить Infrastucture and Process. Сделать короче говоря ETL процесс состоящим из множеств компонентов.
 Этими самыми компонентами являются: Process Repository, Metadata Repository, Scheduling, Process Orchestration, Integration Adapters of Channels, Service and Process Instrumentation and Collection.
 Имеем на 2015 год: 
  1) HDFS and MapReduce - The Core.
  2) Flume - Streaming event data integration.
  3) Sqoop - Batch exchange of relational database channels.
  4) Oozie - Process integration and basics scheduling.
  5) Impala - Fast analysis of data quality.
 Иерархия HDFS : /intent/category/application(optional)/dataset/partitions/files.
 ETL состоит из Tier'ов.
 Структура Tier'ов выглядит примерно так: 
  1) Tier 0 - Row data from source system.
  2) Tier 1 - Derived from 0, cleansed, normalized.
  3) Tier 2 - Derived from 1, aggregated.

7. Oozie:
 Инструмент для построения workflow.
 Коллекция DAGов организованных в виде Actions которые имеют зависимости друг от друга.
 На XML определяем action и зависимости между ними.
 Узлы в workflow бывают двух типов:
  1) Control node - определяет начало и конец, контролирует исполнение workflow.
  2) Action node - определяет выполнение тасок.
 Могут быть заданы переменные в job.properties и использованы в action node.
 Controle node - Start, fork, join, decision, kill, end.
 Action node - Java/Streamin MapReduce, Pig, Hive, Sqoop, file-system task, Distcp, Java programs, shell-scripts, HIIP, eMail, Oozie sub-flow.
 Oozie должен знать о том что было совершено успешно действие, для этого Actions совершают callback (уникальный URL) or polling.
 Actions бывают в двух состояниях: ok and error. 
 Actions бывает двух типов: MapReduce, Scripts.
 Другие элементы способствующие выполнению конкретных Actions:
  1) Prepared elements - лист путей, которые будут созданы или удалены перед выполнением.
  2) Job-xml elements - ссылается на jobConf.xml связанный с workflow app.
  3) Configuration Element - содержит свойства для Hadoop job.
  4) The file and Archive elements.
 Oozie предоставляет набор констант(KG, GB and etc) и функций для манипулирования данными и не только(trim(), datetime() and etc).
 Так же Oozie поставляет набор функций для доступа к данным workflow and Actions(id, name, user, errorCode).
 Так же Oozie имеет набор констант связанных с Hadoop(MAP_IN, MAP_OUT, RECORDS and etc).
 Так же Oozie поддерживает набор HDFS функций(exist, isDir, dirSize etc).
 workflow.xml - xml с заявленным workflow в нем. Должен находится в HDFS.
 job.properties - файл с конфигурацией для конкретной job, пути до workflow.xml, расположение NameNode and DataNode, так же тут определены переменные которые используется в workflow.xml. Может находится и в HDFS и в local File System.
 Coordinator поддерживает автоматический старт workflow. Обычно используется для проектирования и выполнения повторяющихся исполнений триггернутых по времени или дате. Позволяет задать условие(доступность данных, время, внешний ивент) по которому будем запускать workflow. Так же позволяет определить зависимость между workflow jobs, которые запускаются регулярно даже в разное время, путем определения данных которые будут выходном одного, а входом у другого. Конфигурирутся с помощью coordinator.xml, располагающийся в HDFS. Имеет собственные константы и функции (номер дней в месяце) обычно связанные с временем. Так же можно создавать раз в течении какого-то промежутка времени датасет, с помощью шаблона, хранимого в конфигурации cordinatoe.xml. 
 BigInsights Workflow Editor - помогает конфигурировать Workflow графически, избегая конфигурирования из xml. Созданные таким образом потоки могут планироваться, связываться.

8. Luigi:
 Гоняет батч данные.
 Типичный датапайплайн гоняющий через разные операции.
 Пример пайплайна: Взять логи, сагрегировать логи, загрузить логи.
 Так же можно просматриваться работы с помощью schelduer.
 Для решения проблемы c тем когда падает поток следует обернуть(wrapper) целую работу (к примеру прописать условие, которое будет проверять есть ли файл, и только потом передавать работу дальше).
 Task - задание которое выполняет операцию. 
 Fetch logs task - должна иметь output and run method.
 Aggregator task - должна иметь output, run and requires.
 Таска считается завершенной только когда будет выполнен output method (local fs, Hive).
 Таски в которых нет метода output, должны переопределять complete.
 Requires - метод, который указывает зависимости для данной таски (те все таски которые должны быть запущены до выполнения этой таски).
 Run - метод, который указывает на то, что будет делать таска.
 Можно создавать таски для работы нa HDFS.
 Существует великое множество реализованных тасок (запись в разные БД, чтение с файла и тд).
 Нужно заметить то что Luigi не запускает таски за нас или не помогает с параллелизацией работы. Легчайший путь это обойти: триггерить с Python процесса или cron процесса.
 Можно работать с: HDFS, MySQL, Postgres, Pig, Redshift, Spark, BigQuery and more.
 Target - Места с которых данные считываются.
 Основывается больше на том что-то считывает -> записывает, другое повторяет то же самое и тд.

9. Airflow: Уже сдавал экзамен.

10. Azkaban: 
 Фреймвор для планирования потоков  и их разработки. Разработан для управления работой на Hadoop ETL процессов. Все конфигурируется на Java.
 Создан для обработки батч данных.
 Получает порядок через зависимости указанные в коде.











