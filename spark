Structure Streaming:
 Думая об этой концепции важно понимать, что она построена на той основе того, что при разработке Streaming процесса не нужно думать о том что это streaming, все вычисления будут происходить в той же манере что и на обычных типах данных.
 Streaming processing на основе Spark SQL. Главная идея в том, что Streaming построен на основе Dataset и Dataframe. Вычисления происходят так же на там же spark SQL движке, что приводит к импруву при выполнении. Exactly-one и fault-tolerant система. По дефолту Structure Streaming использует micro-batch для доставки сообщений, гарантируя exactly-once, fault-tolerance. С версии Spark 2.3 было введено Continuous Processing, который постоянно поставляет сообщения с гарантией at-least-once.
 К Structure Streaming можно относиться как к таблице, которая постоянно насыщается данными[Input Table]. По сути введена концепция того, что вы оперируете на данных, как на обычных DF или DS.
 Input table:
  1) Каждые пришедшие новые данные, рассматриваются как новая row.
 Жизненный цикл SS: Input table -> Computation on input table -> Result table -> output.

 OutputMode:
  1) Append()[Без агрегаций] - Добавляет только новые строчки в sink. All synk.
  2) Complete()[С Агрегацией] - Добавляет все строчки в sink. All except file synk.
  3) Update()[И то и то] - Добавляет только обновленные строчки в sink, а иначе действует как Append при отсутствии агрегаций. Может использоваться только с ForEach sync and console sync.

 Общая структура такова, что читаются данные из source, считаются и записываются в Result table, а затем Source data дискардится. Spark Streaming держит минимальное количество промежуточных данных.
 
 Event-time - время, когда были созданы данные, не путать с Procesing-time - время, когда данные были приняты Spark. 

 Fault-tolerance - с ней Spark справляется путем того, что он перезапускает или перерабатывает данные. Предполагается что каждый source имеет offset. Движок использует checkpoint and write-ahead log для записи данных которые будут записаны на каждый trigger. Sink в свою очередь гарантирует idempotent, таким образом есть end-to-end exactly-once semantics.
 
 Читать данные можно из (out of the box): File Source [text, CSV, JSON, ORC, Parquet], Kafka source, Socket source (for testing) and Rate source (for testing). Некторые source не fault-tolerance, так как не поддерживают Spark checkpoint. 

 Sources as files по дефолту не поддерживают наследование схемы. Это уверяет в том, что согласованная схема будет использоваться, даже в случае ошибок.
 Partitioning - это когда директории представляются в виде /key=value/...  и Spark на базе колонок пользователя может разместить данные сам в соответсвующие части. !!! Важно помнить, что менять структуру key=value нельзя после того как запустилось Stream приложение, можно только добавлять.

 На SS доступны почти все те же самые операции что на DS and on DF.

 Fault-tolerance - постигается тем что у нас хранятся все офсеты, и если на каком-то шагу у нас упали вычисления, то они начнутся с самого надежного офсета и перевычислятся. [WAL]. Так же еще достигается и с помощью того, что у нас есть более надежное БД, которое хранит состояние. Состояние вместе с WAL могут быть использованы для того чтобы пересчитать данные, чтении которых было нарушено из-за падения работы.

 Window:
  Event-time позволяет нам выполнять window операции, такие как, количество эвентов за минуту. По идеи, Window операции, то же самое, что и обычный group by with agreagation, можно считать каждый window группой, но записи может принадлежать разным группам. Более того, из-за того что Structure Streaming имеет полный контроль над Result table, мы можем либо сохранить, либо выкинуть запоздавшие данные, это называется watermark.
  При создании Window параметризуется время, определяющее как часто и на сколько нужно создавать окна, и имя колонки.
  val windowedCounts = words.groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  ).count()
  Watermarks:
   Позволяет трекать event time приходящих данных и чистить старое состояние если то потребуется. Для определения Watermarks мы должны указать event-time колонку и время, через которое состояние будет очищаться из памяти: df/ds.withWatermark("timestamp", "10 minutes"). Все получается таким образом, что данные попавшие в рамки watermark агрегируются к своему окну, а данные не попавшие в watermark просто удаляются.
   В зависимости от output mode, мы либо Update Result table при каждом тригере, либо Append.
   Для использования watermarks:
    1) Использовать output mode: Append or Update.
    2) Агрегация должна иметь event-time column.
    3) withWatermark должно быть вызвано на той же колонке, что и groupBy.
    4) withWatermark должно быть вызвано до агрегаций для watermark.
  И последнее, watermark с задержкой в 2 часа, гарантирует что данные, пришедшие менее чем за 2 часа, будут обрабатываться, а иначе обратное не гарантируется, те, данные пришедшие более чем за 2 часа, не факт что удаляться, но чем более время задержки, тем больше шанс того что они удаться. 

 Join так же присутствуют, и их результат ничем не отличается от пакетных джоинов. Джоинить как можно статические данные, так и другие стримы. Присутствует множество видов join. Stream-stream работает хорошо, но есть проблема, так как данные могут приходить не согласованно, она решается автоматически, тем, что прошедшие данные хранятся в буфере. Время хранения можно настроить с помощью watermarks. 
  Inner-join:
   При работе с ним нужно учитывать то, что количество данных которое хранится в памяти будет постоянно расти, так как множество колонок из прошлого будет совпадать с новыми колонками. Для того чтобы это избежать, нужно добавить условие, которое запретит совпадение бесконечно старых колонок с новыми:
    1) Определить watermark для обозначения того, на сколько сильно могут задержаться данные.
    2) Определить условие, при котором движок будет понимать, что данные уже устарели и значит их не нужно использовать.
  Outer join:
   Пока для Inner Join использование ограничение является не обязательным, для outer join, вы должны его использовать. Для того чтобы в будущем движок для генерации NULL знал, когда Input ROW не будет совпадать с чем-либо в будущем. Есть еще одно ограничение, говорящие о том, что при использовании Outer join, важно понимать то, что если таблица, которая участвует в join не получила данные, ждущая таблица, выведет результат с задержкой.
  Последние детали о join:
   1) Мб каскадный: df1.join(df2, ...).join(df3, ...).join(df4, ....)
   2) Запросы можно выполнять только в Append режиме.
   3) Вы не можете использовать другие non-map-like операции до join.

 Continuous Processing:
  Стрим с задержкой в 1ms and at-least-once fault-tolerance guarantees. Для активации такого вида стрима, мы должны активировать trigger(Trigger.Continuous("1 second")). Это означает, что ход выполнения запроса будет записываться каждую секунду.

 Catalysis компилирует Spark SQL программы в RDD. Из-за того что он знает: все типы данных, схему наших структур данных, имеет детальное понимание вычислений, которые мы хотели бы сделать. Это делает для нас возможным делать вычисления с возможностью reordering, fuse, уменьшить количество данных которое мы должны прочитать и не выполнять код на не нужных partitioning.

 Tungsten - off-heap data encoder:
  1) highly-specialised data encoders: Так как он знает схему данных он может  хранить данные в более упакованном виде. Это значит больше данных может быть упаковано в память.
  2) column-based format: хранить данные в колонках гораздо выгоднее, так как почти все запросы происходят с ориентированием на колонки.
  3) off-heap: хранит данные вне кучи, и управляет ей сам => не нагружается GC.

DataFrame:
 Имеется такой метод, как na (not avalible) - который определяет DataFrame с пустыми значениями в row. На этих волокна можно вызвать drop, fill and replace (не только на пустых колонках). Чтобы обратится к вложенной колонке, мы можем использовать Person.name. withColumn - позволяет добавлять или заменят колонки, при этом если уже колонка существует, то она заменится. spark.sql("SELECT * FROM parquet.`data/result`") - пример считывания данных с папки. Show(trunkate = false) - запрещает обрезать колонки при их выводе. newSession - создает новую сессию из уже существующей SparkSession.
 C Spark 2.x была введена поддержка SQL 2003, поддержка subquery и все работает на нативном SQL parser. import org.apache.spark.sql.functions._ импортирует вспомогательные sql функции для манипуляции над данными внутри spark. groupBy на DataFrame куда безопаснее чем на RDD, так как тут происходит работа оптимизатора. При работе с вложенными структурами, важно помнить о наличие методов для обработки вложенных структур.
 Explode - удаляет колонки, которые содержать null value, но explode_outer сохраняет.
 Window - это некая абстракция, которая выделяет под собой какой-то промежуток строчек и выполняет операции на них. Window.partitionBy($"AccountNumber").orderBy($"Date").rows/rangeBetween(-4, 0) - тут определяем по какой колонке строится окно, потом свойства, а затем range.  Функция first, выводит самый первый результат базируясь на группе, если группировки не было, то на всем DF или DS. Udf Не видимы для оптимизатора, так что он не сможет их оптимизировать. При использовании join если у 2 таблиц разные имена колонок, то мы можем их сджоинить $"id1" === $"id2", а иначе надо использовать df1("id") === df2("id") или Seq("id"). 

DataSet: 
 Для того чтобы все еще пользоваться плюсами Catalysis, мы должны использовать только Relational функции. 

DStream:
 Самый первый вид Streaming на Spark, представляет собой поток бесконечно идущих rdd, создающихся за какой-то промежуток времени. Концепт вычисления очень похож на обычный RDD, это задумано для того чтобы переход между концепциями был гладок.
 При создании StreamingContext, если уже существует SparkContext, передавайте его в параметрах, и время которое определяет как часто создавать батч данных.
 Вся обработка и прием едет параллельно, то есть батчи друг от друга независимы.
 Для создания какой-либо параллельной коллекции, мы можем создать список RDD, который потом закинуть в DStream, и каждый интервал DStream будет читать по порядку RDD.
 Можно явно указать, чтобы DStream хранил данные после их вычисления, по дефолту он их сразу удаляет, если неявно не указано то, что они нужны потом. Добиться этого можно с помощью ssc.remember(...). testStream.slice(from,to) - возвращает нам все RDD, которые приходили от from до to определенного промежутка времени. count and countByValue возвращает нам количество значений в RDD в первом случает, а во втором как и количество, так и значения, в виде пары.
 Все трансформации происходят по таким же методам что и с обычным RDD(map, reduce, cache and etc.). Большинство методов применяются на каждый RDD отдельно. Есть метод transform который позволяет нам выполнять всякие вычисления на каждом RDD отдельно.
 Так же существуют window операции, которые происходят по какому-то промежутку времени(3s) и создаются за какой-то промежуток времени(2s). Причем два параметра должны быть кратны batch interval. Из себя представляет операции над попавшими в окна RDDs для производства новых RDDs. Здесь так же присутствует такая фишка, что первые окна будут создаваться с таким же как и slide interval временем, в тех случаях, если window interval больше или равен slide interval, это сделано для того, чтобы охватить все данные.
 Checkpoint обычно требуется для разных stateful операций, для того чтобы избежать data lost. Позволяет нам откатится в случае падения до состояния когда был все хорошо, и пересчитать данный, так же ладе позволяет пересоздать driver в случае падения. Можно прокинуть checkpoint в StreamingContext для того чтобы Driver смог восстановиться, такой же логики примеривается и DStream. Slide interval мб не установлен, он будет равен размеру батча. UpdateStateByKey - интересная функция с той точки зрения что она позволяет нам обновлять состояние со всех данных которые приходили до этого, для ее существования необходимо нахождение checkpoint директории. mapWithSate - обновляет DStream в соответсвии с ключем, значением и состоянием. Из личного наблюдения то, что данные во втором случае приходят только обновленные, в то время как в 1 случае, все.

Write Ahead Log (WAL) - fault tolerance концепт, суть которого заключается в том, что данные перед тем как попасть на обработку, записываются в логи. И таким образом если драйвер при собирании матча упадет, он сможет считать логи которые пишет сам с логами Receiver и восстановить данные.

Direct Steam - Stream без Reciver, используется при считывании с Kafka, его плюс в том, что можно создать столько partition, сколько будут читать с разных partition у topic. Так же у нас нет WAL, так как у Kafka и так есть checkpoint. Exactly-once semantics достигается тем, что Spark сам хранит offsets.

Для того чтобы улучшить выполнение Join, модно broadcast не большие таблицы, для того чтобы избежать shuffle.

Для стадии shuffle, Spark производит map and reduce операции, которые сначала собирают данные, а потом с помощью reduce аггрегирют. Внутренне map записывает записи в память если сможет уместить и затем сортирует в рамках одной partition, и reduce уже читает и агрегирует эти блоки. Сама по себе shuffle напрягает memory, так как организует в памяти данные перед отправкой, а если данные не помещаются в memory, то вовлекается диск, а тут проблема IO and GC. Так же происходит засерание памяти за счет того что много данных остаётся после shuffle и GC собирает только в случает того, когда GC запускается или никакой RDD не использует эти данные. Spark автоматически сохраняет некоторые промежуточные данные при shuffle, чтобы пересчитать в случае падения.

Плюсы переноса данных через Broadcast вместо того чтобы переносить с таской в том, что он применяет эффективный broadcast algoritm и уменьшает цену коммуникации. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.
Accumulator может обновлять значения только в actions.

Для того чтобы преобразовать RDD в Dataset, можно наследовать схему с помощью reflection, если уже RDD был параметризован. Так же можно сделать схему путем программирования.

Interesting tips:
Можно вызывать coalesce для того чтобы уменьшить распределение данных и для чего улучшить производительность.
Для улучшения производительности можно броадкастить какие-то переменные, например при join two tables.

Spark tuning:
Можно сериализовать данные или десериаилизовать для импрува GPU, memory or network. По умолчанию используется Java Serialization, которая в разы медленнее чем Kryo Serializtion. Но последняя из них требует ручного вмешательства для кодирования.
При работе с памятью следует учитывать то, что у нас есть три главные проблемы: количество данных которые мы хотим хранить, способ доступа к ним и перегрузка garbage collector.
К сожалению объекты в Java очень многословные, например очень много занимают такие вещи как хедер объекта, его внутренние данные, например у LinkedList, мы храним очень много ссылок, а у String, каждый символ в UTF-8 и так же length есть. Коллекции примитивных типов хранят их в коробках.
Memory usage в Spark состоит из двух частей: execution and storage. Execution это та память которая используется для shuffles, joins, sorts and aggregations. Storage memory refers to that used for caching and propagating internal data across the cluster. Интересно то, что если у нас нет какой-то из этих частей, то другая занимает всю доступную память. С другой стороны Ex может выселить St, до какой-то определенной trash holder,  который определен количеством закодированной памяти. Приложение которое не использует cache, может посвятить все память для execution.
Для импрува модно использовать какие-то свои классы без излишних ссылок, или не использовать тяжелые объекты где это применимо. Если наши объекты все таки не влазят в память, их можно сериализовывать.
GC является большой проблемой, так как ему надо постоянно смотрен через все объекты, мы можем помочь ему если будем хранить наши объекты в серилизованном виде, или не создавать много сложных объектов. Если у нас слишком много раз запускается GC, то вероятнее всего не достаточно памяти на машинах. Если слишком много раз запускается minor GC, то это значит что надо увеличить Edem memory. 
Для улучшения производительности очень поможет точная установка уровня параллелизма.
Стоит внимательно следить за Data Locality, Spark всегда сначала попытается расположить код прям там где у нас есть ресурсы, но если ресурсы заняты, он ждет и если устал ждать, то он пойдет на уровень locality ниже.












