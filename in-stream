```Kafka
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 1      - создание топика
kafka-topics.sh --zookeeper 127.0.0.1:2181 --list   - лист топиков
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --describe    - описание топика

kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic   - запустить консольного producer [--producer-property (acks=...)]
kafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topic --property parse.key=true --property key.separator=,     - producer key/value pairs с разделеителем ,

kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic    - создание консольного потребителя, причем он читает только новые сообщения [--from-beginning, --group ...]
kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning --property print.key=true --property key.separator=,    - consumer key/value pairs с разделеителем ,

kafka-consumer-group --bootstrap-server 127.0.0.1:9092 --list    - список групп
kafka-consumer-group --bootstrap-server 127.0.0.1:9092 --describe --group my-second-application    - описание группы по имени
!!! С Kafka >= 2.1 при вызове kafka-consumer-group "console-consumer" группа больше не появляется !!!

kafka-consumer-groups --bootstrap-server 127.0.0.1:9092 --group my-second-application --reset-offsets [--to-earliest, ...] --execute --topic first_topic    - сдвинуть offset для данной группы и данной группы по определенному параметру

kafka-configs --zookeeper 127.0.0.1:2181 --entity-type topics --entity-name configure-topic --describe      - описание конфига топика 
kafka-configs --zookeeper 127.0.0.1:2181 --entity-type topics --entity-name configure-topic --alter --add-config min.insync.replicas=2       - добавляет минимально количество реплик в конфиграцию.
kafka-configs --zookeeper 127.0.0.1:2181 --entity-type topics --entity-name configure-topic --alter --delete-config min.insync.replicas      - добавляет минимально количество реплик в конфиграцию.
```

Apache Kafka with Spark:
 На своей стороне Apache spark и Kafka являются fault-tolerance продуктами, так как Spark имеет возможность переобработать ошибочные данные благодаря тому что Kafka хранит их у себя какое-то время.

 1) Apache Kafka по своей архитектуре состоит из кластера брокеров, каждый из которых состоит из очередей или топиков, в свою очередь каждый из которых состоит из нескольких partitions. И каждый partitions реплицируется на брокеры, которые ответственны за них, в соответсвии с фактором реплики. Все это сделано для durability и контролируется с помощью ZooKeeper.
  Здесь всегда есть некий компромисс между latency and the system’s level of durability. К примеру, мы можем продюсеру отправлять незамедлительный ack (без реплики), когда данные достигнут Kafka, что будет опасно в случае падения брокера, тогда данные потеряются, или ждать когда данные скопируются, и только тогда сдать ack.
  Очень важным является выбрать подходящее количество partition, так как это очень болезненный процесс и требует ручного вмешательства, так что лучше сделать это раз и на долгое время. Про выборе partition важно понимать что, слишком большое количество приведет к тому, что у нас будет перенагружены брокеры и zookeeper, с другой стороны, данные будут быстрее читаться из spark. Короче говоря, при выборе partition очень важно смотреть на следующие показатели: 1) Необходимый уровень параллелизации, чтобы не простаивал Spark. 2) Возможность справится с возросшей нагрузкой без repartitioning. 
  Что касается железа, при работе с Kafka, так как он оптимизирован для эффективного вычисления и I/O. Это критически удачно распараллелить I/O, так как это самая медленная часть системы. Так же нужно достаточно памяти, для того чтобы Kafka успешно хранил в себе пришедшие данные.
  В общем, Apache Kafka позволяет нам разделить источник и приемник сообщений, так как без   Kafka, почти не реально поддерживать такую бесконечно растущую систему.
  Kafka может быть использован в: Messaging System, Activity Tracking, Gather metics from many different location and etc.
  И важно помнить, что Kafka лишь доставляет сообщения, без вычислений.

  Компоненты Kafka: 
   1) Topic - конкретный поток данных. Похоже на таблицу в БД, топиков может быть несколько и идентифицируетя по имени. Сообщения в topic остаются immutable.
   2) Partition - это то, на что поделен topic. Каждый partition упорядочен и каждое сообщение внутри partition имеет свой растущий id, называемый offset. Само по себе id сообщения не имеет значения, только в связке с topic. Другими словами, offset имеет значение только в пределах partition. Данные в partition являются неизменяемыми. Данные назначаются сюда рандомно до тех пор пока не будет предоставлен key. Каждый key идет в тот же partition.
   3) Broker - можно рассматриваться как сервер. Каждый брокер имеет ID и определенное количество topic partition. Когда коннектишься к какому-то Broker, по существу коннектишся к целому кластеру, так как каждый broker знает о другом broker, topics и partitions. Это позволяет нам не заботиться о том, к какому брокеру или partition цепляться. 
   4) Kafka cluster - состоит из Broker.
  При создании кластера, топик распределяет свои partition по всем брокерам.
  По дефолту, данные в Kafka хранятся в пределах 1 недели, потом они удаляются.

  Преимущества Kafka: 
   1) Не читает и на считает данные (No GPU usage).
   2) Kafka берет байты как входной поток без того чтобы даже загрузить их в память, просто отправляет дальше.
   3) Распределяет данные.
   4) Kafka даже не знает какого типа наши данные.

  Replication - Каждый топик должен быть скопирован для fault-tolerance в том случае, если упадет Broker, обычно от 2 до 3. Благодаря этому, если упадет 1 брокер, другой может подхватить данные. Важно понимать: Только один брокер может быть лидером у partition => только лидер может получать и обрабатывать данные для partition. Другие брокеры лишь синхронизируют данные, следовательно каждый partition имеет одного лидера и многих ISR. Лидер и ISR определяются с помощью ZooKeeper, если лидер упадет, он передаст лидерство реплики, но как вернется, заберет лидерство обратно. Безумно важно сделать верный partition с первого раза, нарушению порядку ключей и не менять replication factor, так как иначе это может вести к неожиданным результатам на performance и нагрузке на cluster. Большее количество Partition подразумевает больший параллелизм, возможность запускать больше consumers, возможность задействовать больше брокеров, но идет нагрузка на ZooKeeper и больше файлов открывается на Kafka. Хорошей практикой является устанавливать количество реплик минимум 2, обычно 3, максимум 4. RF конечно увеличивает надежность, но сильно увеличивает нагрузку на диск и high latency, особенно если ack=all. Если RF это проблема, то всегда лучше установить надежные broker.
  Каждый Partition строится из Segments, и последний segment называется как active segment. Сегменты в свою очередь просто файлы. Данные пишутся в момент времени ток в один сегмент, как он переполнится, откроется новый. Для конфигурирования размера сегмента у нас есть две опции: log.segment.bytes - максимальный размер сегмента в байтах, log.segment.ms - время в течение которого Kafka будет ждать, прежде чем создать новый сегмент. Сегмент образуется двумя файлами: 1) An offset до позиции, с которой Kafka будет читать. 2) A timestamp для offset index, позволяет нам найти сообщение с timestamp. Поэтому Kafka знает где найти данные за constant время.
  
  Producer пишет данные в topic, он автоматически знает в какой broker and partition писать. В случае падения Broker, Producer автоматически восстановится. Он автоматически баллансирует нагрузкой, шлет данные частями в разные partition (в случае прихода данных без ключей). 
  Producer может выбрать получать или нет ack о том что были данные получены, есть 3 уровня  ack:
   1) 0: Без разницы на ack (могут быть потеряны данные).
   2) 1: Отзыв от брокера о том что было получено сообщение (ограниченная возможность потери сообщения).
   3) all: Отзыв от брокера и реплик о том чтоб было получено сообщение (без потери сообщения).
  Так же он может выбрать, слать или нет key с сообщением, это может быть: str, number или что угодно. Если ключа нет, то данные будут слать по очереди и кругу каждому брокеру. А с ключом, все данные с соответствующим ключей будут присылаться в один и тот же partition. Обычно ключи обозначаются в том случае, если нам надо обозначит порядок в по определенному полю. Распределение по partition идет благодаря key hash.

  Consumer читает данные из топика. Он заведомо знает из какого broker читать. В случае падения broker, он consumer будет автоматически восстановлен. !!! Данные читаются в порядке в пределах каждого partition !!! Данные читаются параллельно. Consumers читают данные в группах. Каждый Consumer в пределах группы читает с exclusive partition, это значит что есть consumers больше чем partitions, то некоторые потребители будут бездействовать. Назначение потребителей к partitions будет происходить автоматически с помощью: GroupCoordinator и ConsumerCoordinator. При его создании мы читаем только новые сообщения, но при помощи опции (--from-beginning), можем читать все.

  Kafka хранит offset на которых consumers group читает данные. Такие offset'ы хранятся в Kafka топик под названием __consumer_offsets. Из этого следует, что когда Consumer прочитал данные, он должен отправить Kafka считанный offset. Это все сделано для того, чтобы consumer мог продолжить работу, если он упал.

  Delivery Semantics - Потребители выбирают когда выполнять commit. Есть 3 вида Delivery Semantics:
   1) At Most once - offset are committed как только сообщение было получено.
   2) At least once - offset are committed после того как сообщение будет обработано (тут нужно быть уверенным с тем, что наше сообщение не было обработано дважды).
   3) Exactly once - только для Kafka -> Kafka workflow и Kafka -> External System с использованием idempotent системой.

  Когда мы подключаемся к Broker, мы прикидываем connection + metadata req, в ответ получаем List of all brokers. И в конце можем подключиться к нужным broker'am.

  При создании консольного producer, который будет писать в топик мы можем написать топик, которого не существует, но это не будет ошибкой, для нас создастся специально новый топик. С дефолтными параметрами в server.properties, которые при желании можно поменять.

  Kafka client конвертирует все value and keys в поток байтов, который посылается в Kafka, поэтому нужно определять serializer. И обратно так же при считывании дисериализует.

  Client Producer должен конфигурировать ack вместе с min.insync.replica=2 - подразумевает что по меньше мере две реплики должны ответить с тем что у них есть данные. Producer не пытается пересылать не передавшееся сообщение бесконечное количество раз, у него есть лимит в несколько миллисекунд[delivery.timeout.ms] и количество раз[retries], который можно настроить, так же можно настроить частоту пересылки. При пересылке, данные могут прийти вне очереди, если не настроить количество параллельных подключений к partition [max.in.flight.requests.per.connection].
  Idempotent producer - это Producer который недопускает поставку дубликатов из-за разрыва сети, вместе с сообщением он посылает его id, и если брокер замечает дубликат, он просто его не принимает, но заново принимает ack. Чтобы его включить, нужно написать enable.idempotence = true.
  Compression так как зачастую данные тяжелые, то можно применять метод компрессии на приходящие строки. Она особенно помогает в тех случаях, когда приходит большое количество данных. Из плюсов то, что у нас теперь больше места, и больше данные быстрее проходят, и минусов то, что CPU тратится на де и компрессию.
  Если Broker не успевает потреблять сообщения, то Producer будет их сохранять в себе. Если количество записей превысит то сколько их может храниться в Producer, то операция send будет блочиться. 

  ClientConsumer может быть At least once, At most once, Exactly once. Kafka Consumer работает по модели pull, что значит что он сам решает когда и сколько данных ему нужно взять, и сколько он их будет ждать, так же есть еще и push модель. Для контролирования взятия данных можно настроить fetch.min.bytes (1) - контролирует какое минимальное количество данных consumer готов взять за запрос. max.poll.records (500) - контролирует максимальное количество записей, которые consumer готов прочитать. max.partitions.fetch.bytes (1MB) - максимальное количество данных возвращенное через брокер по partition. fetch.max.bytes (50MB) - максимальное количество данных которые могут прийти за запрос.
  Есть две стратегии при Commit offset:
   1) (easy) enable.auto.commit = true & синхронная обработка commit. При этом commit совершается каждые 5 секунд (auto.commit.interval.ms).
   1) (medium) enable.auto.commit = false & ручная обработка commit. Мы должны сами вызывать consumer.commitSync(). 
  Для того чтобы ускорить процесс обработки информации, можно потреблять данные не по одному, а целыми батчами.
  Если Consumer считывает данные которые, уже залежались больше чем 7 дней, то offset потеряется, для избежания такого, мы должны настроить параметр auto.offset.reset [none, earliest, latest] (бросает исключение если offset не найден, начинает считывать с начала логов, начинает считывать с конца логов)
  Время хранения consumer's offset может быть настроено с помощью offset.retention.minutes.
  При чтении из Broker Consumer не только pull данные, но и шлет сердцебиения к Consumer Coordinator. Если определяется что Consumer умер так как не присылает более сердцебиение, то происходит rebalance. session.timeout.ms (10 sec) - показатель того, через какое время без сердцебиения Consumer считается мертвым. heartbeat.interval.ms (3 sec) - показатель того, как часто нужно отправлять сердцебиение. max.pull.interval.ms (5 min) - показатель того, через какое время между двумя pull запросами, consumer будет считаться мертвым. Особенно важно за этим следить в Spark приложениях, так как обработка бывает очень долгой.
  
  Bacthes, по дефолту Kafka читает сообщения по одному, как можно быстрее, но при этом если приходит слишком много, он их объединяет в Batch. linger.ms - параметр который указывает на время, которое данные будут собираться, по дефолту 0. batch.size - размер batch, если он переполнится до истечения linger.ms, то Producer отправит пакет незамедлительно. Batch size указывается per partition. 

  При хешировании ключей, а точнее их двоичного представления, Kafka использует murmur2 алгоритм. Есть возможность переписать дефолтный partitioner [partitioner.class].

  Kafka Connect упрощает получение и вынос данных из Kafka. И позволяет трансформировать данные внутри Kafka без использование external lib. По сути используем уже написанные за нас Connectors. Например Twitter connector, который сам за нас читает данные с twitter и прогоняет в Kafka.
  Kafka Connect and Stream API: Sources => Connect Cluster => Kafka Cluster => Stream Apps  (work with data) [our code] => Kafka cluster => Worker => Sink. Для построения такого pipeline, нам нужно использовать Source / Sink connectors.

  KafkaStream - Java библиотека для data processing и tranformation на Kafka. У них так же есть application id, по логике похоже не группы у Consumer.

  SchemaRegistry это отдельный компонент, с ним должны общаться Producer and Consumer. The Schema Registry должна иметь возможность отвергать данные. Общий формат данных должен быть согласованным: 1) Это должно поддерживать schema. 2) Это должно поддерживать evolution. 3) Это должно быть lightweight. Использует Apache Avro как формат данных. Producer шлет schema to Schema Registry и шлет данные в Kafka, затем Consumer берет данные из Schema Registry и из Kafka. И schema в данном случае является неким гарантом того, что у нас пришли верные данные.

  При создании кластера нужно создать несколько копий ZooKeeper, причем чтобы он был разделен от Kafka, и брокеры должны быть равномерно распределены между машинами.

  Kafka Metrics - Kafka внутри себя использует JMS, и через нее выставляет свои метрики, Эти метрики очень важны, для того, чтобы быть уверенным в том что наша система ведет себя корректно под нагрузкой, общее место для хранения метрик: ELK, Datadog, NewRelic, Confluence Control Center, Promotheus and etc.
  Набор важных Metric:
   1) Under Replicated Partition - проблемы у partition с ISR, может быть свидетельством того что какой-то брокер работает не правильно и высокой нагрузки на систему.
   2) Request Handlers - переиспользование потоков из-за больших чтений и записей, можно решить увеличинеми CPU.
   3) Request timing - мониторить задержку.

  Security: Encryption - в Kafka уверяет то, что у данные между клиентом и брокером засекречены. Authentication - только подтвержденные клиенты могут использовать Kafka. Authorisation - говорит о том, какие может и какие нет действия совершать наш пользователь. 

  При построение Kafka cluster для крупных компаний, ее кластеры создаются по всему миру и обмениваются друг с другом репликами. Есть два вида построения кластеров: 1) Active => passive - потребляем активными и копируем в пассивный (some aggregation, disaster recovery strategy, cloud migration) 2) Active => Active - потребляем обоими и обмениваемся данными (global data and global dataset). 

  Log Cleanup Policies: множество Kafka кластеров make data expire в соответствии этой самой policy. По дефолту поведение стоит на delete. Поведение compact, на __consumer_offset - удаляет базируясь на ключах сообщений и удалит старые дубликаты ключей после active segment is committed. Log cleanup происходит на partition segment. Smaller / More segment значит что log cleanup будет происходит чаще => больше требоваться от CRU and RAM. Cleaner следит за работой каждые 15 секунд. log.retention.hours - показатель который говорит о том сколько времени будут задерживаться сообщения при показателе на log cleanup = delete. log.retention.bytes [-1] - показатель о том как много данных будет удерживаться для каждого partition. 

  Log compaction уверяет то что наш partition хранит только последние значения для каждого ключа. Offset у сообщений are immutable. Так же удаленные сообщения будут видны в течении какого-то времен delete.retention.ms[24 hours]. Можно его вызывать руками через API, но лучше такого не делать.

  unclear.leader.election - опция которая позволяет нам копировать сообщения даже если ISR упали, но при этом мы теряем данные.

  При подключении к Kafka Client использует public IP, Kafka отвечает ему с тем чтобы Client использовал ADV_HOST, и если Client использует его и не находит его в сетях, то подключение не проходит, а иначе проходит.

  Kafka Connect: Каждый connector переиспользуемый кусок кода. Причем Connector + User Config = Tasks. И каждая Task выполняется с помощью Workers. Worker - один java процесс. Запускать работу Kafka Connector можно в 2 видах: 1) Standalone - один процесс обрабатывает все таски. Только для разработки. 2) Distributed - множество процессов на кластере. Для продакшена. В данном виде процесса, если какой-то worker отвалится, то работа передастся другому worker.
  При запуске connect-standalone worker.properties file-stream-demo-standalone.properties важно понимать что первым параметром должен быть конфиг worker, а вторым конфиг стольких в коннекторов скольких захотим.
  Общение происходит посредством REST API.
  Config Def - это те самые параметры которые используются для связи с пользователем.
  Kafka connector - отвечает за получение конфигов из config и создание tasks. 
  Schema - описание того, как данные должны выглядеть. Это использует примитивные типы, а затем Kafka Connect конвертирует это в JSON и AVRO.
  Source Partition - позволяет Kafka Connect знать о том, какой source вы чиатли.
  Source Offset - позволяет Kafka Connect отслеживать до скольки вы читали source partition.
  Когда connector принимается кластером, workers перебалансируют все connectors and their tasks. То же самое происходит когда количество tasks у connector уменьшается или увеличивается, или когда connector's configuration изменяется. Когда падает task, никакой другой worker ее не берет, это рассматривается как исключительный случай. Failed tasks не перезапускаются автоматически.

  Flafka - это Kafka Sink, Channel and source в Flume. С версии Flume 1.7 можно передавать Events вместе со schema. В Flume 1.7 сообщения приходить стали ассинхронно, до этого было синхронно, хотя это был настраиваемый параметр. Flume поддерживает Kafka security. Было возвращено использование свойства "request.required.acks" в тех случаях, когда не используется "requiredAcks", раньше было только "requiredAcks". Так же Flume 1.7 поддерживает Kafka 9.0 и более поздние версии.

 2) ZooKeeper позволяет достигнуть высокой доступности для Apache Kafka partitions and brokers, as well as a resource manager for the Spark Streaming cluster. Позволяет распределенным системам следовать за сервисами, хранит их состояние и выбирает лидера. Он состоит из нескольких узлов, которые хранят транзакции и информацию об узде, ему почти не нужна вычислительная мощь и RAM. Однако он очень зависит от Disc I/O, следовательно, нам хорошо бы оставить посвященную машину под него, или участок памяти. !!! От ZooKeeper зависит работа Kafka !!!
  
  Хранит все brokers вместе, помогает выбрать лидера для partition, шлет notifications to Kafka, когда произошли какие-то изменения. По дизайну он оперирует только на нечетных числах 3,5,7. Zookeeper имеет под собой лидера(управляет записями) и followers(управляют чтением)(метадата). Но ZooKeeper уже не хранит данные об consumer offset с Kafka > 0.10. 
 
 3) Spark должен обладать zero data loss, no duplicates, and automatic failover for all component-level failures. Со стороны Kafka мы абсолютно точно получаем at Least Once, из-за того что, если что-то упадет, то Kafka просто перешлет эти сообщения в виде микро батча. Exactly one должен быть достигнут либо как я делал в проекте либо с помощью downstream tolerant. Spark Streaming сейчас становится все популярнее.
  Spark Session - по факту обертка для SparkContext, SQL and Hive Context.

Ignite:
 A horizontally scalable, fault-tolerant distributed in-memory computing platform for building real-time applications that can process terabytes of data with in-memory speed.
 Он может использовать RAM не только для кеширования, но и для того чтобы там хранить данные. Пользователь может включить и выключить persistence, если будет включено, то Ignite becomes a distributed, horizontally scalable database that guarantees full data consistency and is resilient to full cluster failures, а иначе Ignite can act as a distributed in-memory database or in-memory data grid. 
 Ignite Native Persistence is a distributed, ACID, and SQL-compliant disk store.
 With Ignite Persistence enabled, вам больше не надо заботится об обертке данных и индексов после того как кластер перезапустится, ведь вы не обязаны хранить данные и индексы только в памяти, Durable Memory is tightly coupled with persistence and treats it as a secondary memory tier. И следовательно если какой-то поднабор данных или индексы сотрутся, Durable Memory will take it from the disk.
 Data stored in Ignite is ACID-compliant both in memory and on disk, making Ignite a strongly consistent system. Ignite transactions work across the network and can span multiple servers.
 Ignite provides full support for SQL, DDL and DML, allowing users to interact with Ignite using pure SQL without writing any code.
 The in-memory data grid component in Ignite is a fully transactional distributed key-value store that can scale horizontally across 100s of servers in the cluster. When persistence is enabled, Ignite can also store more data than fits in memory and survive full cluster restarts.
 В отличие от других БД, Ignite позволяет перемещать легкие вычисления на данные, таким образом collocating computations with data. Как результат, Ignite быстрее расширяется и минимизирует перемещение данных.
 Ignite гибкий и позволяет работать на кластере, который можно не надобности расширять или уменьшать. Так же можно хранить несколько копий данных, что позволяет говорить о fault-tolerance. If the persistence is enabled, then data stored in Ignite will also survive full cluster failures. При перезапуске кластера Ignite может сразу начать работу, так как загружать данные в память не надо, он будет их туда лениво подгружать.
 Является in-memory distributed key-value store, может рассматриваться как, partitioned hash map, где каждый cluster node обладает частичкой памяти, таким образом, чем больше мы машин в кластер добавим, тем больше данных сможем сохранить.
 Не похоже на другие key-value хранилища, Ignite determines data locality using a pluggable hashing algorithm.

 Ignite provides three different modes of cache operation: PARTITIONED, REPLICATED, and LOCAL. A cache mode is configured for each cache.
  Partitioned - дефолтный способ хранения данных, данные просто разделяются на части и частями равномерно сохраняются на компьютерах, что позволяет нам говорить о том, что весь кластер может рассматриваться как один большой датасет. Имеет преимущество в обновлении, так как обновлять прийдется только один node и в лучшем случае его backup соратника. С другой стороны очень затратное чтение. In order to avoid extra data movement, it is important to always access the data exactly on the node that has that data cached. This approach is called affinity collocation and is strongly recommended when working with partitioned caches. !!! Подходит для хранения большого количества данных и частого обновления !!!
  REPLICATED копирует все данные между всеми нодами, имеет очень быстрый доступ к данным, но падает perfomance на том, что данные при обновлении приходится обновлять и во всех копиях. !!! Идеален для случаев, когда у нас маленький датасет, и обновления происходят не часто !!!
  Local самый удобный для всех видов операций. Обладает всеми преимуществами Partition, только проигрывает в том, что если cluster упадет, то все данные потеряются.
 
 Используются в связи со Spark с преимуществом в in-memory store and computing, все происходит быстро и почти без задержки. Так же взаимодействует как и с RDD, так и с DataFrame.
 IgniteRDD представляет собой некую распределенную таблицу в Ignite. Он может быть создан в пределах Spark Job или на раздельном Ignite cluster. От этого зависит сколько будет жить эти данные. Так же были привнесены первичные и вторичные индексы через Ignite, что позволило выполнять Spark работы быстрее. Это видение кеша прямо сейчас, что значит то, что любые изменения внесенные в данные лежащие в ignite будут тут же отображены в IgniteRDD. RDD partition создается в том же количестве что и Ignite partition, так же предоставляется в Spark информация о том где лежат данные, для того чтобы производить вычисления прям на данных. Из-за того что данные в IgniteRDD не подгружаются, а и пользуются прям в Ignite, то нам не нужно даже их подгружать, можно производить вычисления сразу. При сохранении данных в Ignite, можно использовать пары RDD, по принципу key/value, так же можно сохранить просто value, ключ будет генерироваться сам. Эти операции можно выполнять параллельно. На базе IgniteRDD можно выполнять SQL запросы.
 The Apache Spark DataFrame API introduced the concept of a schema to describe the data, allowing Spark to manage the schema and organize the data into a tabular format. Вместе с этим привносятся все фишки Catalysis. Ignite расширяет Spark DataFrame, что ведет к более простой разработке и удобному доступу к ней, включены следующие бенефиты: 1) Возможность шарить данные между Spark job, записывая и читая DataFrame с Ignite. 2) Более быстрое выполнение с помощью Ignite SQL engine which include​ advanced indexing and avoid data movement across the network from Ignite to Spark. Filtering выполняется на стороне Ignite, остальное на стороне Spark, отсюда седует что данные все таки грузятся в Spark. Записать в Ignite DataFrame можно способами: Append, Overwrite, ErrorIfExist, Ignore. Если при записи DF будует создана новая таблица, то следующие опции должны быть обозначины: OPTION_CREATE_TABLE_PRIMARY_KEY_FIELDS, OPTION_CREATE_TABLE_PARAMETERS.
 
 Сам по себе состоит из нескольких слоев, один из которых compute grid, занимается вычислением, как Spark. Data grid - отвечает за хранение.

 Есть несколько видов деплоя: Node Singletone - когда деплоится сервис на все node, Cluster Singletone - когда деплоится сервис на один node, но если он упадет, то сервис передеплоится на другой.

 С Apache Ignite 2.0 данные и индексы хранятся off-heap. Support for DDL and alter indexes. 

 При включенной опции Ignite percistance, все данные хранятся на диске, а в памяти хранится по возможности, самые требуемые.
 Providing proper cache store implementation is important whenever read-through or write-through behavior is desired. Read-through means that data will be read from persistent store whenever it’s not available in cache, and write-through means that data will be automatically persisted whenever it is updated in cache.










 




