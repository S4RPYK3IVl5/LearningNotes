class MyClass(x: Int, y: Int) {           // Defines a new type MyClass with a constructor  
  require(y > 0, "y must be positive")    // precondition, triggering an IllegalArgumentException if not met  
  def this (x: Int) = { ... }             // auxiliary constructor   
  def nb1 = x                             // public method computed every time it is called  
  def nb2 = y  
  private def test(a: Int): Int = { ... } // private method  
  val nb3 = x + y                         // computed only once  
  override def toString =                 // overridden method  
      member1 + ", " + member2 
  }
new MyClass(1, 2) // creates a new object of type


val pairs: List[(Char, Int)] = ('a', 2) :: ('b', 3) :: Nil
val chars: List[Char] = pairs map {
  case (ch, num) => ch
}

Весь код пишется в файлах с расширением .scala.
Скомпилированный код сохраняется в файл с расширением .class.

Common Issues
 #1 Avoid Casts and Type Tests
  Никогда не используйте isInstanceOf or asInstanceOf.
 #2 Indentation
 #3 Line Length and Whitespace
 #4 Use local Values to simplify complex Expressions
 #5 Choose meaningful Names for Methods and Values
 #6 Common Subexpressions
  Никогда не вызывайте тяжеловесные функции по несколько раз.
 #7 Don’t Copy-Paste Code!
 #8 Scala doesn’t require Semicolons
 #9 Don’t submit Code with “print” Statements
 #10 Avoid using Return
 #11 Avoid mutable local Variables
 #12 Eliminate redundant “If” Expressions

Functional programming:
 В таком стиле программирования нам нужно бы было ввести новые абстракции, которые бы расширяли предыдущие, и нужно бы было добавить новые теории к этим абстракциям;
 Mutation - изменение чего-то при этом сохраняя идентификатор таким же.
 Theory - не поддерживает мутацию.
 Для примера String в Java, который не предоставляет операторов для изменения своей внутренней структуры.
 Функциональное программирование создано чтобы избежать мутацию.
 Мы должны избавиться от всяких условий, циклов и других императивных структур контроля.(restricted)
 В широком смысле смотреть на такой вид программирование как на программирование на функциях.(wide)
 В частности, функция может рассматриваться как значение которое создается, потребляется и составляется.
 Функции в ФП являются первоклассными гражданами, которые имеют все права что и переменные в императивном стиле, могут передаваться в функции и возвращаться, могут создаваться везде где угодно, так же есть набор операторов для составления функций.

Не примитивные выражения происходят слева на право в соответствии с важностью оператора.
Когда мы используем переменные в операциях, вычисление происходит таким образом что заменяется на выражение, которое стоит справа от объявления переменной, и это выражение исполняется пока не получится значение. 2*pi*radius => 2*3.14*radius => 6,28*radius => 6.28*10 => 62.8

Объявление переменной : val a = 10. Сразу читает значение. Call-by-value.
Объявление определения : def x = 10. Создает переменную, которая рассчитывается только тогда, когда вызовут. Могут являться функциями def square(x: Double): Double = x*x. Так же можно заставить функцию применить этот способ, путем передачи в аргументы параметр вида: y: => Int. Call-by-name.
Lazy val x - выполняется однажды когда нужен.
Выполнение функций в Scala  приохотит таким же образом что и операции, сначала вычисляются значения аргументов слева на право. Затем заменяется функция в месте вызова с помощью кода который определен в ней, а затем заменяются другие переменные, объявленные в функции. Такая схема выполнения называется как substitution model. Все выражения стремятся стать значениями. Такая модель дала основание для ФП. Такой тип выполнения может быть применим к выражениям без побочного эффекта(с++);
Существует такая теорема что, если CBV завершается, то CBV так же завершится. Обратная теорема не верно.
В Scala обычно используется СВV. Так как, как правило, все аргументы вычисляются.
Рекурсивные выражения в Scala должны иметь явное возвращаемое значение.

Примитивные типы такие же как и в джава. Только с большой буквой в начале. 

Существует такие выражения как if-else (в Scala оно представлено в виде выражения, а не утверждения). 
Булевы выражения такие же как и в Java. Short-circuit evaluation используется в булевых выражениях, когда есть смысл не вычислят левое выражение, если правое говорит об однозначности.

Blocks in Scala по сути является неким местом выражений, в котором последняя строчка является тем, что вернет блок после исполнения. Блок определяется фигурными скобками, он появляется везде где может появится выражение, так как сам таковым является.
Данные определенные в блоке, являются част только этого блока, и затемняет все заявленные из вне переменные с тем же именем.

Точка с запятой нужны только в том случае если у нас несколько утверждений записаны в 1 строчку.

Tails recursion - вид рекурсии при котором у функции последним действием вызывается она же, и для вычисления этой функци используется такой же стек вызовов. Можно сказать это альтернативная формулировка итеративного процесса (выполняется как цикл). Так же для такого вызова функции не обязательно вызывать себя же, можно и другую функцию. Но в Scala поддерживается вызов к только такой же функции, и запрашивается аннотация @tailrec.

Higher order functions - функции, которые принимают в качестве параметра функции или отдают другие функции в качестве возвращаемого значения. def sum(f: Int => Int)(a: Int, b: Int): Int = { if (a > b) 0 else f(a) + sum(f)(a+1,b) } - функция, принимающая функцию, возвещающая функцию, возвращающая Int.
First order function - функции, которую принимают и возвращают только обычные типы данных. Объявление принимаемого значения в виде функции выглядит таким образом sum(f: Int => Int).
First-class function - это все функции в Scala так как они соотносятся в коде как обычные переменнные.

Anonymous functions - функции без имени, как лямбда выражения в Java. (x: Int) => x*x*x тип параметра мб опущен, если тип значения возвращаемого значения можно определить во время компиляции. 

При определении функции которая определяет функцию вида sum(f: Int => Int): (Int, Int) => Int = {...}, мы можем избежать возвращения функции, путем передачи в возвращающую функцию параметров, пример sum(x => x*x) (10, b).
Currying - стиль ФП, при котором у нас вызов происходит таки образом sumf(12)(35)(65).
ФП является право ассоциативным, значит что (Int => Int) => (Int, Int) => Int == Int => Int => Int.

Type : Int and etc, Boolean, String, function type.
Expression : identifier(x, isGoodEnought), literal(1, 1.0, "abc"), function application(sqrt(x)), operator application(a+b, -c), selection(math.abs), conditional expression(if ... else ...), block, anonymous function(a => a*a).
Defenitio n: val, var, def. 

Class - создает новый тип в Scala, со своими специфичными свойствами. C помощью constructor мы можем создать элемент определенного типа (object) путем написания new Claszz(...).
Members - члены класса.
Methods - функции в пределах класса.
Private - модификатор который скрывает какой то элемент класса от клиента этого класса.
This - ссылка на текущий объект.
Require(...,...) - конструкция внутри класса, которая объявляет условия которые должны быть выполнены, а иначе бросает IllegalArgumentException. Используется для навязывания клиенту условия пользования.
Assert(...,...) - то же самое что и require(), только бросает AssertionError. Используется для проверки кода функции.
Primary constructor - по сути Scala уже неявно предлагает инстанс конструктора, который параметры класса, и выполняет все выражения в body of class.
def this(x: Int) = this(x, 1) - определение второго конструктора для класса.

Data abstraction - это когда мы изменяем поведение класса, но при этом поведение клиента не меняется.

Инстанс класса создается по тем же правилам что и вызов функции.
При чейне функции и создании объекта через new, сначала заменяются параметры у функции а затем у объекта.

Operators - оперторы в Scala можно переопределись (rational1 + rational2)
Infix notation - каждый метод с параметром может быть записан как: x add y.

Операторы могут быть использованы как идентификаторы, таким образом они могут быть: 
 Alphanumeric - начинается с буквы, с последующими буквами и цифрами.
 Symbolic - начинается с символа, с последующими операторами символами. def - (...) = ... . Обязательно нужно ставить пробел между объявлением операторов, так как сплина будет считаться как часть имени оператора.
 Нижнее подчеркивание, считается как буква.
 Alphanumeric могут состоять из нижнего подчеркивания и символами операторов.
Преимущество операторов определятся тому какой оператор стоит первым в названии. Каждый объявленный новый символ сильнее заявленных в Scala изначально.

Abstract class - имеют привычный функционал что и абстрактные классы в java. Могут иметь методы которые не содержат реализации. Мы не можем создать инстанс класса через new.
Class A extends B {...} - пример расширения класса.
IntSet -> (Empty, NonEmpty)
IntSet - superclass
Empty/NonEmpty - subclass
BaseClasses - все родители определенного класса.
Все классы в Scala наследуются по крайней мере от одно класса: Object.
Если есть желание переопределить методы, которые уже реализованы в родители нужно добавить слово override к определенному в родителе def ... .

оbject - ключевое слово, которое создает синглтон класс. Не может существовать второго инстанса класса. По сути ведет себя как статический класс. Технически он создается как только мы его вызываем. Ссылаемся на него только по имени. 

Dynamic method dispatch - механизм, из-за которого происходит определение реализации метода прямо в runtime, те метод вызываемый по ссылке определяет к какому типу он принадлежит, и исходя из этого выбирает реализацию.

Persistent data structure - фид структуры данных, при котором все изменения в структуре данных приводят к созданию новой версии структуры данных, а старая версия все еще поддерживается. 

Package - пакует класс в иерархию папок.
Import - импортирует пакет с классами. _ - обозначает то что импортируем все из заданного пакета. {... , ...} - импорт нескольких классов из пакета. Импортировать можно не только классы но так же и методы объектов. Некоторые базовые члены импортируются автоматически.

Trait - то же самое что и интерфейс в Java, обладает теми же свойствами. Но traits могут иметь поля, в то время в Java нет, но с другой стороны в traits нельзя создавать values.

Общая идея с классами и полям такая же как и в Java, собственно можно наследоваться от 1 класса, но от нескольких интерфейсов.

scala.Any - базовый тип для всех классов в Scala.
scala.AnyVal - базовый тип для всех примитивов в Scala (Integer, Float and etc).
scala.AnyRef - базовый тип для всех ссылочных типов данных. Alias of java.lang.Object.
scala.Null - сабтип класса scala.AnyRef (по сути просто null is instance of scala.Null). Совместим со всеми классами которые наследуебтся от класса AnyRef, не совместим с классами от AnyVal.
scala.Nothing - это subtype scala.Any который обозначает отсутсвие инстансы любого объекта. Он применяется в методах, которые должны выбрасывать Exception. Так же представляет тип Nil. Создан для того чтобы удовлетворить JVM. Представляет пустую коллекцию.
Nil - обозначение листа, который не имеет элементов. object Nil extends List[Nothing].
Unit - сабтип scala.AnyVal по идеи обозначает функцию, которая не возвращает значение. Он не представляется никаким объектом в рантайме.
None - Option ни с чем. case object None extends Option[Nothing]

throw Exc - выбрасывает exception, тип такого exception: Nothing. 

Innutable list - фундаментальная структура данных для многих ФЯП. Он состоит из Nil(пустой лист), Cons(элемент вместе с остатком листа).

class Rational(x: Int, val y: Int) - разница в объявлении в том что x - параметр, а y - параметр и поле класса одновременно. Val определение это специфический случай определения функции (они могут переопределять методы и тд).

Дженерики:
 class MyClass[T](arg1: T) { ... }  
 new MyClass[Int](1)  
 new MyClass(1)   // the type is being inferred, i.e. determined based on the value   arguments
 Параметризовывать методы так же можно. 
 Object не может быть параметризован определенным типом.
 type erasure -  означает то что наши типы параметров зачищаются перед тем как начнет выполняться программа. Это означает что параметр нужен только для времени компиляции для проверки правильности нашей программы, но во время выполнения это просто не нужно.

Полиморфизм:
 Означает то что функция мб применима к аргументам многих типов. 
 Существует два главных вида полиморфизма; 
  1) Subtyping - инстанс subclass может быть передан везде где принимается superclass.
  2) generics - параметризация класса или метода.

Вообще существует 3 типа: примитивный, функциональный и классовый.
На первый взгляд Scala не различает что Int, Double, Boolean and etc. от других классов. Но по причинам эффективности Scala compiler отображает эти классы в присущую им память как в java. 

Функции в Scala соотносятся как объекты. Пример функция вида A => B : trait Function[A,B] {def apply(x: A): B} - получается функция это всего лишь объект тип Function с методом apply, существуют функции с 22 параметрами.
Функция типа f(a,b) будет представлена в виде f.apply(a,b).
Если есть что-то представленное в виде def y(...) = {...} - это не будет функциональным значением. Но если такая функция понадобится в качестве возвращаемого значения, то она будет сразу сконвертирована в функциональное значение.

Мы видели два типа полиморфизма, эти два типа могут взаимодействовать 2-я способами: bounds and variance.
Liskov principle - если A<:B, тогда все, что работает с типом B так же должно работать и с типом A.
Bounds:
 Upper bounds - def myFct[T <: TopLevel](arg: T): T = { ... } // T must be a subtype of or be a TopLevel
 Lower bounds - def myFct[T >: Level1](arg: T): T = { ... }   // T must be a supertype of Level1
 def myFct[T >: Level1 <: TopLevel](arg: T): T = { ... }
 The term B >: A expresses that the type parameter B or the abstract type B refer to a supertype of type A. In most cases, A will be the type parameter of the class and B will be the type parameter of a method.
Variance: 
 Covariant - тип, для которого справедливо следующее List[T] <: List[TopLevel]
 Массивы в Scala не covariant.
 Типы, которые immutable могут быть covatiant (List), а обратно нет (Array).
 Если C[T] and A <: B:
  C[A] <: C[B] - covariant (должны появляться только в результате метода)
  C[A] >: C[B] - contravariant (должен появляться только в принимаемых аргументах метода)
  Ne ither - nonvariant (может появляться везде где угодно)
 class List[+A] [Covariance] - обозначает что класс, параметризованный определенной ссылкой, может быть приведен к тому же классу параметризованному потомком класса от параметра ссылки. (val animals : List[Animal] = List[Dog](...))
 class Printer[-A](x: A) [Contravariance] - будет обозначать то, что ссылка этого класса параметризованная определенным классом может быть приведена к типу объекта этого класса параметризованным супертипом класса параметра ссылки. (val x: Printer[Dog] = new  Printer[Animal](new Animal))
 class Container[A](value: A) [Invariance] - обозначает то что класс может быть параметризован только одним классом и не быть приведен к параметру какого либо другого.(val catContainer: Container[Cat] = new Container(Cat("Felix")))
 If A2 <: A1 and B1 <: B2, then 
 A1 => B1 <: A2 => B2
 Функции должны быть contravariant в их типе аргументов и covariant в их типе возвращающего параметра.

А Классе Any ест методы: 
 isInstanceOf[T]: Boolean - повреяет своместим ли тип с параметризованным типом.
 asInstanceOf[T]: T - приводит переданный класс к типу параметра.

Decomposition не всегда лучшее решение проблемы.

Case class - еще один способ создать класс, который не имеет тела. Scala автоматически добавляет apply метод для назначения параметров для класса и создания инстанса.

Matcher:
 (someList: List[T]) match {
   case Nil => ...          // empty list
   case x :: Nil => ...     // list with only one element
   case List(x) => ...      // same as above
   case x :: xs => ...      // a list with at least one element. x is bound to the head,
                            // xs to the tail. xs could be Nil or some other list.
   case 1 :: 2 :: cs => ... // lists that starts with 1 and then 2
   case (x, y) :: ps => ... // a list where the head element is a pair
   case _ => ...            // default case if none of the above matches
 }
 Match - это по типу switch в java or C.
 Он может искать совпадения по объекту и классу его представления.
 def eval(e: Expr): Int = e match {
  case Number(n) => n
  case Sum(e1, e2) => eval(e1) + eval(e2)
 }
 Общий вид - cases, pat => expression.
 MatchError выбрасывается если не один паттерн не совпал с случаем.
 Pattern часть состоит из классов, переменных, wildcards, constant. Одно и тоже называние переменно может появляться только 1 раз в pattern, т.е. Sum(x,x) не пройдет. Все переменные должны начинаться с маленькой буквы, а константы с большой. 
 Match подгоняет паттерны по порядку. Как только нужный pattern нашелся, весь блок match переписывается в выражение которое находится справа от pattern.
 Mecth'ing происходит 3 способами:
  1) Совпадение класса C(p1,p2,p3,...,pn) со всеми паттернами содержащими класс C или его subtypes с таким же количеством элементов.
  2) Переменная x найдет совпадение с каждым элементом, и приведете найденное имя к значению x.
  3) C, как константа совпадает со значением, которое найдено в pattern.

List:
 Быстрый доступ к голове элемента, но медленный к середине или концу.
 Реализует класс Seq.
 Способ хранения не совсем удобен так как данные хранятся в разных местах, ведь связанны они друг с другом через ссылки. Очевидно хорошо для рекурсивных задач.
 Фундаментальная структура данных которая представленна в виде x1,x2,...,xn : List(x1,x2,...,xn). 
 В отличие от Array, List неизменяемый и рекурсивный. В List элементы должны быть однородные.
 List состоит из Nil == List[Nothing] и элементов соединенных операцией (cons) x :: xs, где x - head листа, а xs его голова. :: - по конвенции имен в Scala, выполняется справа налево.
 Все известные операторы могут быть выражены с помощью head, tail and isEmpty.
 Может быть использован в match конструкции, может быть декомпозирован по:
  1) Nil
  2) p::ps - элементы и хвост (1 :: p2 :: ps)
  3) List(p1, p2, ... , pn) - совпадение по листу с заданным количеством элементов.
 Основные свойства:
  length - длина листа
  last - последний элемент
  init - лист элементов листа кроме последнего
  take n - взять n элементов листа
  drop n - дропнуть первые n элементов листа и взять остаток
  (n) - взять n-ый элемент
  head - взять первый элемент
  tail - взять все кроме первого
  ++ - конкатенирует два листа.
  reverse - резервирует список.
  update(x,y) - возвращает новый лист где элемент с индексом x заменяется на y.
  indexOf x - берет индекс элемента
  contains x - проверяет на наличие элемента в списке
  concat xs - складывает листы, замена ему ++
  ::: - конкатенирует листы
  :: - конкатенирует элемент с листом
  splitAt n - возвращает пару листов, в которой первый лист представляет элементы с 0 до n, а остаток оставшиеся элементы после n.
  map p - изменяет лист по какому-то критерию с каждым элементом листа.
  filter p - выводит лист который удовлетворяет всем условиям.
  filterNot p - обратноке методу filter.
  partition - выполняет две функции выше и возвращает их как пару (filter, filterNot)
  takeWhile p - берет список пока он удовлетворяет условию.
  dropWhile p - то же что и сверху только наоборот
  span p - выполняет две функции выше и возвращает их как пару (takeWhile, dropWhile)
  reduceLeft - вставляет переданные оператор между элементами листа. x, y => x + y.
  foldLeft - на нем определен reduceLeft, он принимает накопитель в качестве параметра. Который возвращается как только оно дойдет до Nil. Может работать с Nil. Вся ветка на выходе будет иметь тип параметра который имел нулевой элемент.
  reduceRight - то же что и reduceLeft, только идет справа на лево.
  foldRight - аналогично выше, только для foldLeft
 left... and right... операторы у листа с для коммутативных и ассоциативных операций работают одинаково.
 Можно записать вместо (x, y) => x * y : _*_, где _ представляет слева направо аргументы в функции.

Pairs and Tuples - по сути создает пары, которые состоят из типизированных элементов (x,y). Так же может быть больше чем два элемента. (T1, T2, ..., Tn) является аббревиатурой Tuplen[T1, T2, ..., Tn]. val (label, value) = pair - можно задать имя значениям пары таким образом.
val pair = ("answer", 42)   // type: (String, Int)
val (label, value) = pair   // label = "answer", value = 42  
pair._1 // "answer"  
pair._2 // 42 


Ordering - Аналог Comparator в java.

Implicit - позволяет нам не прикидывать нужный параметр в функцию, компилятор это сделает за нас. Путем проверки типа передаваемого значения. Если не было подобранно наиболее проходящее значение, то будет выбрано исключение.

Natural induction -  свойство, цель которого доказать, что свойство удовлетворяет всем наборам значений, к примеру: для всех N>B натуральных чисел будет будет справедлив inductional step n + 1 и это новое значение будет включено в в этой свойство.
Factoral(n) => power(2, n), для всех n >= 4. - свойство
n + 1 - inductional step.

Structural induction - подтвердить что свойство P(xs) справедливо для него листа xs. Nil - base element. Induction step - если P(xs) - справедливо, то для элемента x справедливо P(x + xs). 

Vector:
 Так же immutable.
 Реализует класс Seq.
 По сравнению доступ к элементам более сбалансированный. По существу очень маленькое дерево.
 Если в нем 32 или меньше элементов то он представляется в виде Array. если становится больше чем 32, то мы получаем вектор из 32 ссылок на массивы по 32 элемента, и тд.
 Очень хорош для bulk операций, требующих прохождения по последовательности, к примеру map filter или fold.
 Способ хранения здесь так же выигрывает, так как данные хранятся рядом в массиве.
 +: или :+ - добавить элемент влево или вправо списка.
 При создании вектора у нас не создается весь вектор заново, перерождаются те элементы, которые мы поменяли, и те которые ссылаются на измененный, вплоть до головы, а на неизменные части голова нового вектора ссылается.

Iterable:
 Базовый класс для всех коллекций (Seq, Set, Map).

Seq:
 Наследуется от Iterable.
 Является базовым классом для многих коллекций.
 Методы: 
  exist p - проверяет есть ли элемент в коллекции.
  forAll - проверяет все ли значения подходят данному предикату.
  flatMap - предоставляет collection - value функции ко всем элементам в коллекции, и конкатенирует результат, создавая новую коллекцию. Аналогично: map.flatten
  zip - создает коллекцию пар из двух коллекций. Коллекцию с большими элементами отрежет.
  unzip - обратное к zip. Вернет одну пару с двумя листами. 
  sum - сумма элементов числовой коллекции.
  product - умножение всех элементов в числовой коллекции.
  max - находит самый большой элемент(Ordering должен существовать)
  min - Так же что и max, только наоборот. 
  flatten - соединяет все списки в списке в один список. 
  sortWith - принимает два аргумента и правило по которым ему сортировать.
  sorted - сортирует по готовому методу Ordering по параметру
  groupBy - формирует список ассоциативных массивов, по свойству

Arrays and String - имеют все те же методы что и Seq. Они неявно конвертируются в коллекции где нужно, они не подклассы Seq, так как они берутся из java.

Range:
 Представляет последовательность равномерно распределенных чисел.
 Наследуется от Seq.
 Представляет собой объект, имеющий всего 3 поля: lower bound, upper bound, step. Может быть создан с помощью: 1 until 5, 1 to 5, 1 to 10 by 3, 6 to 1 by -2.

В функцию map можно вкладывать pattern matching - map{ case ... => ... }.

Collections
 Base Classes
  Iterable (collections you can iterate on) 
  Seq (ordered sequences)
  Set
  Map (lookup data structure)
 Immutable Collections
  List (linked list, provides fast sequential access)
  Stream (same as List, except that the tail is evaluated only on demand)
  Vector (array-like type, implemented as tree of blocks, provides fast random access)
  Range (ordered sequence of integers with equal spacing)
  String (Java type, implicitly converted to a character sequence, so you can treat every        string like a Seq[Char])
  Map (collection that maps keys to values)
  Set (collection without duplicate elements)
 Mutable Collections
  Array (Scala arrays are native JVM arrays at runtime, therefore they are very  performant)
  Scala also has mutable maps and sets; these should only be used if there are performance   issues with immutable types

For-Expression - синтаксический сахар для foreach, map, flatMap, filter or withFilter операций на тех класса которые их реализуют.
 for (x <- 1 to M; y <- 1 to N)
   yield (x,y)
 (1 to M) flatMap (x => (1 to N) map (y => (x, y)))
 for(s) yield s
 Состоит из generator, filter and e.
 Generator - генератор последовательности, может быть добавлено несколько генераторов.
 Filter - фильтрация идущая после *generator* if *filter*.
 e -  выражение которое выведет после отработки того что в for. 
 Вместо круглых скобочек можно использовать квадратные, чтобы опустить использование точки с запятой при переносе скобки.
 Каждый следущий генератор в for быстрее предыдущего.
 По сути является for циклом, который сохраняет результат в буфер, и выводит его в ту коллекцию, по которой итеррировал.
 Так же может быть использован в виде pattern matching.
 По существу эквивалентен запросам в RDBMS.
 Работает таким образом что постепенно уменьшается до простейшего вида for(x <- ...) yield  ex.
 For expression может работать не только с коллекциями, его работа базируется на присутствии в классе методов map, flatMap, withFilter.

Set:
 Immutable
 Создается так же как и Seq, большинство операторов с Seq, так же присутствуют и на Set.
 Порядок в Set неопределенный
 Не имеет дубликатов
 Главная операция : contains

Map:
 Immutable
 Такой же map как и везде, создается с помощью Map(1 -> "a", 2 -> "b") and etc.
 Map расширяет Key => Value функцию, что позволяет ей использоваться везде где нужна функция. 
 map(1) - пример доступа к элементу, но если так вызывать, выбросится Exception.
 get obj - позволяет взять объект так же как и выше, но возвращает Option.
 withDefaultValue - превращает map в map с дефолтным значением, если элемент не найден, что делает ее total function, a не partial function.

class Option[X] очень похож по своей сути на java class Optional.
Имеет два значения:
 None - отсутствие значения (option[Nothing])
 Some(x) - присутствие значения
Так как case class, то можно разложить Some через pattern matching.
Так же саппортит flatMap, filter, map операции.

Repeated parameters - def this(bindings: (Int, Double)*) - позволяет вводить в конструктор произвольное количество пар.

Разница между filter and withFilter в том, что filter является командой, которая основана на strictness коллекции, в то время как withFilter является независимой. Разница в том что первый из них генерит новую коллекцию, а второй является неким фактором который пропускает или нет данные. 

________________////////////////______________________-------------____________________

Ассоциативность операторов определяют по их последнему символу.

Так же можно написать таким образом: map {case (...) => ...} где преобразования будут применяться к каждому элементу подходящему по шаблону. Само по себе выражение не типизировано, следовательно нужно предписать ожидаемый параметр.

От функций можно наследоваться таким образом: trait Seq[Elem] extends Int => Elem, теперь мы можем писать elems(0).

PartialFunction - аналог Function1, за исключением того что он применим к тем функциям, в которых используется pattern matching. С помощью функции isDefinedAt(). 

Monad:
 По существо параметризованный тип M[T] котором есть такие функции как flatMap[U](f: T => M[U]): M[U] {} и unit[T](x: T): M[T]. И эти функции должны удовлетворять неким правилам. Map для каждого Monad мб разный в зависимости от того как комбинировать flatMap and unit.
 Тип, которой хочет рассматриваться как Monad, должен удовлетворять 3 правилам:
  1) m flatMap f flatMap g === m flatMap (x => f(x) flatMap g) [Associativity]
  2) unit(x) flatMap f == f(x) [Left unit]
  3) m flatMap unit == m [Right unit]
 Типы которые еще и определяют метод withFilter, называются Monads with zero.

Try - это класс которой создан для того чтобы передавать исключения между вычислениями, похож на Option.

Structural induction - такой вид индукции применим не только к листам но и к любой древовидной структуре.
Общий принцип индукции таков, для того чтобы доказать, что свойство P(t) применимо к любым деревьям t конкретного типа нужно доказать следующие правила:
1) Показывает что свойство P(l) применимо ко всем листьям l дерева t.
2) Для каждого внутреннего узла дерева t, с поддеревьями s1, s2, s3 ... sn. Справедливо следующее: P(s1) union P(s2) union P(s3) union ... union P(sn) = P(t).

Stream - то же самое что и List, только его хвост исполняется только по требованию.
В стримах отсутствует такая операция как ::, вместо этого существует #::. 

Здесь выступает та проблема, что у нас каждый раз перевычисляется хвост при его вызове. Но это можно избежать, если использовать lazy evaluation [lazy val]. Scala по дефолту использует Strict evaluation. Фактически м можем быть носителями бесконечной последовательности.

Помимо того, что в Scala тип может наследоваться от значения, так же может быть и наоборот. Хорошим примером служит, когда Scala сама подставляет определенное значение типа Order.
Implicit аргумент у функции может быть только один и он должен быть последним.
При поиске implicit кандидата, компилятор ищет все параметры параметризованные тем же типом, затем он ищет марку implicit в объявлении и последнее, взятый кандидат должен быть виден с точки вызова. 
def, val, lazy val, or object definition могут быть помечены как implicit. Implicit параметр так же может принимать implicit и обычные параметры.
Сначала ищутся implicit параметры в той же области что и вызов, затем пойдет искать в companion objects associated with T, это означает что пойдет искать по родительским типам.
Пример:
trait Foo[A]
trait Bar[A] extends Foo[A]
trait Baz[A] extends Bar[A]
trait X
trait Y extends X 
If an implicit value of type Bar[Y] is required, the compiler will look for implicit definitions in the following companion objects:
Bar, because it is a part of Bar[Y],
Y, because it is a part of Bar[Y],
Foo, because it is a parent type of Bar,
and X, because it is a parent type of Y.
However, the Baz companion object will not be visited.
Если найдено больше 1 или 0 implicit кандидатов, то Scala бросает исключение. Но если найдено много кандидатов, и один из них наиболее подходящий, то выбран будет именно наиболее проходящий.
Синтаксический сахар:
 def printSorted[A: Ordering](as: List[A]): Unit = { println(sort(as)) } ===  def printSorted[A](as: List[A])(implicit ev1: Ordering[A]): Unit = { println(sort(as)) }.
implicitly[Ordering[Int]] - вызов специфического implicit кандидата для конкретного типа.
def implicitly[A](implicit value: A): A = value - implicit это не спецальное ключевое слово, это библиотечная операция.

Type class:
 Сами по себе типизированные классы, обладают возможностью менять код в зависимости от того, какой тип туда попадет. И для того чтобы спасти такие классы от непредсказуемого поведения, существуют правила которым они должны удовлетворять.
 К примеру, класс Ordering[A]:
  1) inverse: the sign of the result of comparing x and y must be the inverse of the sign of the result of comparing y and x,
  2) transitive: if a value x is lower than y and that y is lower than z, then x must also be lower than z,
  3) consistent: if two values x and y are equal, then the sign of the result of comparing x and z should be the same as the sign of the result of comparing y and z.
Интересным тут может быть то что наши implicit типы могут быть вложенными друг в друга, как например implicit def orderingList[A](implicit ord: Ordering[A]): Ordering[List[A]] = {...}. 

Implicit conversions позволяет использовать неявный конвертор из одних типов в другой. Но перед использвонием этой тулы нужно прописать import scala.language.implicitConversions. Помимо всего прочего, так же можно использовать эту тулу, как добавление функционала к существующей структуре.
1)case class Duration(value: Int, unit: TimeUnit)
object Duration {
  object Syntax {
    import scala.language.implicitConversions
    implicit class HasSeconds(n: Int) {
      def seconds: Duration = Duration(n, TimeUnit.Second)
    }
  }

}
2)import Duration.Syntax._
val delay = 15.seconds
3)val delay = new HasSeconds(15).seconds

Implicit это конечно хорошо но нужно использовать с умом.

В теории лямбд есть такое правило, все замены в функциональном программировании ведут к одному и тому же результату.
Объект обладает состоянием если его поведение меняется со временем.
В Scala у объекта может быть состояние в том случае, если у него есть переменные объявленные с помощью ключевого слова "var". Дальше все как в Java.

Referential transparency: (val x = E; val y = x) == (val x = E; val y = E)
Выражения x и y эквивалентны если ни один тест не может найти различия между ними.
Но в такой ситуации не работает Substitution model так как при написани:
Val x = new BankAccount; val y = x =>  Val x = new BankAccount; val y = new BankAccount, но это не так.

Классический for loop не может быть реализован в Scala, так как в Java он содержит некие объявления. С другой стороны в  Scala можно создать такого вида цикл  for (i <- 1 until 3) {...}.
For-loop рассматривается как for expression, при этом вместо map и flatMap, цикл разбирается на forEach. 
For (i <- 1 until 3; j <- "abc") {...} == (1 until 3) foreach (I => "abc" foreach (j => ...)).

Describe event simulator: Выполняет ивенты юзера в какой-либо момент времени. Action -> функция не примате никаких параметров и ничего не возвращает.

The observer pattern - это паттерн который использовать когда нужно view регирует на изменения в model. Это так же называется как: 1) publish/subscribe. 2) model/view/controller (MVC). Фигурально View subscribe себя на обновления model, а model отвечает с помощью publish.
Плюсы такого паттерна: 
 1) Разделение view и state.
 2) Позволяет иметь разное число views к одному state.
 3) Легко устанавливать.
Минусы такого паттерна:
 1) Императивный стиль.
 2) Очень много чего должно быть скоординированно.
 3) Сложности становятся при многопоточности, когда оба model дергают один view в одно и то же время, может произойти исключение.
 4) View тесно связаны с Model, и если мы хотим чтобы это все происходило более асинхронно, то нам пока это невозможно сделать.

RP - это все о реакции на последовательность ивентов происходящих в определенное время.
FRP - агрегация последовательности ивентов в сигнал.
 1) Сигнал - это значение которое изменяется с течением времени.
 2) Это представление функции от времени к доменному значению.
 3) Вместо того чтобы менять состояние существующего сигнала, мы создадим новый, на базе старого.
Есть две фундаментальные операции на сигнале:
 1) Получить значение сигнала в определенное время.
 2) Получить сигнал из других сигналов.
Так же сигнал можно привести к константе.
Есть еще один вид сигнала, под названием: "Var", он определяет операцию update, которая меняет внутреннее состояние сигнала.
Есть одна огромная проблема при работе с mutable переменными, к примеру, a = 2; b = 2 * a; a = 3 => b - не изменится, из этого следует, что если что-то меняется, нам надо поменять все остальное в ручную, с другой стороны в immutable такой фигни нет.
Важно понимать разницу:
 1) v = v + 1 - производит новую v на основе старой v + 1.
 2) s() = s() + 1 - Возвращает всегда новое значение на базе себя самого + 1.

Каждый сигнал определяет:
 1) Его настоящее значение.
 2) Настоящее выражение которое определяет значение сигнала.
 3) Другие сигналы, которые зависят от него. 
Так же важно понимать, что когда меняется како-либо сигнал, нужно так же менять и другие сигналы, которые зависят от него.
И тут так же может вскочить проблема связанная с тем что у нас мб такое что несколько потоков будет менять caller, и тут приходит на помощь библиотека Scala, которая позволяет нескольких потокам иметь доступ к своей копии коллекции.

________________////////////////______________________-------------____________________

Параллельное программирование - вычисления происходящие в одно и то же время.
Простой принцип - вычисление мб быть разбито на маленькие проблемы, которые будут решать параллельно.
Предположение - у нас есть машины способные вести параллельные вычисления.
Параллельное программирование на много сложнее чем последовательное, так как разделить задачу на под задачи бывает невозможно, и параллельное программирование приводит к новому виду ошибок.
Speedup - единственная причина по которой мы занимаемся параллельным программированием.

Parallel and concurrent программировние очень близко связаны, но в их определении есть некоторые различия.
Parallel - использует параллельное железо для более быстрого вычисления. Эффективность - главный показатель.
Concurrent - как может так и не может вычислять параллельно. Больше о модульности, ответственности и поддержке.

Bit - level parallelism - обработка нескольких битов данных в параллели.
Instruction - level parallelism - выполнение разных инструкций из одного и того же стрима параллельно.
Task - level parallelism - выполнение совершенно разных задач в параллели.
Последний из этого списка достигается с помощью программирования, а первые два с помощью логики процессора.

Operational system - ПО которое управляет hardware and software ресурсами, и управляет выполнением программы.
Process - программа которая выполняется на OC.
Одна и та же программа может выполняться несколько раз или даже параллельно на одной и той же ОС.
Multitasking - это когда процессы постоянно приостанавливаются и продолжаются за какое-либо время[time slice].
Два процесса не могут достучаться до памяти друг друга так как они изолированы.
Каждый процесс может хранить более детальные примитивы, названные как threads.
Потоки создаются внутри одного процесса и так же они шарят общее адресное пространство. Каждый поток содержит свой стек и program counter, который содержит методы которые будут выполняться в пределах потока. Program counter - указатель на текущую задачу. Другой поток не может дотянуться до стека другого.
Каждый JVM процесс начинается с main потока. 
Для того чтобы начать новый поток, нужно - 
 1) Создать потомка Thread
 2) Создать объект потомка
 3) Вызвать start на объекте
Thread описывает код которой поток будет выполнять, так же кастомный поток может выполняться несколько раз в одном потоке.

Методы класса Thread:
 1) start - начать поток выполнения.
 2) join - ждать конца выполнения этого потока.

Операция считается atomic, если с тз других потоков она рассматривается как что-то что произошло мгновенно и не делимо.
Synchronized - гарантирует то что код внутри этого блока никогда не будет выполняться двумя потоками одновременно. Другими словами позволяет достигнуть atomicity. Со стороны JVM это реализуется подтем хранения monitor ни каждом объекте.
Monitor - гарантирует что максимум один поток может владеть объектом.
Synchronized блоки могут быть вложенными.
Deadlock - сценарий в котором два потока забрали себе по ресурсу и ждут пока один освободит другой.
Чтобы отключить возможность дедлока, мы должны лечить в одном и том же порядке.

Java Memory Model - модель, в которой хранится набор правил, которые описывают то, как потоки потоки взаимодействуют, когда трогают общую память.
 1) Блок синхронизации не нужен в том случае когда два потока читают/пишут в разные участки памяти.
 2) 2 поток Х, который вызывает join на Y, видит все записи сделанные Y, после освобождения потока.
Для параллелизма нужно передавать переменные по имени. 

Проблемы:
 1) Не всегда хорошо написанное параллельное вычисление ведет к перфомансу, так как бывает и такое, что все потоки нуждаются в одном и том же массиве, который хранится в RAM, и при этом этот самый юнит доступа к массиву является buttleneck, так как все потоке аксессятся к нему в параллели. 
 2) Нужно уметь правильно распараллеливать потоки, так как если у нас один поток выполняется быстро, а другой медленно, поток который исполнился быстро, будет простаивать и ждать когда второй исполнится.

Task -  позволяет запускать вычисления в параллели и затем с помощью join ждать до тех пор, пока не будет возвращен результат.

Есть 2 способа оценить параллельные вычисления:
 1) Empirical measurement - запуск параллельных вычислений на определенной архитектуре.
 2) Asymptotic analysis - требует понимание того, как меняется перфоманс алгоритма, когда: 
  1) Входные данные становятся больше.
  2) У нас есть больший hardware параллелизм.

Оценить сложность алгоритма в последовательном программировании мы привыкли и с точки зрения того, сколько у нас происходит операций.
W() - оценка работы в последовательном выполнении
D() - оценка работы в параллельном выполнении
P - количество процессов
Сложность алгоритма равняется : D() + W()/P

Testing - быть уверенным с тем что все части программы работают в соотвествии с желаемом повелением.
Benchmarking - высчитывает метрики производительности(время исполнения, memory footprint, metric traffic, disk usage or latency) для частей программы.
Между ними есть одна существенная разница, testing выдает обычно true или false, а benchmarking выдает постоянно обновляемое значение.
Performance (time) зависит от многих факторов: 
 1) Processor speed.
 2) Количество процессоров.
 3) Доступ к памяти и пропускная способность.
 4) Cache behaviour.
 5) Runtime behaviour.

Измерять улучшения обычно трудно, так как, мера улучшения зачастую случайное число. Например, два запуска программы могут иметь похожий, но разный показатель времени исполнения.
С первого взгляда может показать что идеальным способом будет просто провести несколько раз один и тот же запуск, а затем посчитать среднее, но этого может быть недостаточно, хорошей практикой будет: исключить те данные, которые являются аномальными, выключить GB, JIT compilations and aggresive optimisation. или является ли наша программа уже оптимизирована.

List не подходит для параллельный вычислений, так как его сложно поделить на половины и комбинировать.
Лучше использовать использовать альтернативы: arrays and trees.
Arrays:
 - :
  1) Нужно убеждаться в том что мы не пишем данные в одни и те же участки
  2) Очень долго конкатенировать
 + :
  1) Очень быстрый доступ и использование массива относительно памяти.
Trees:
 - :
  1) Требуется очень много памяти.
  2) Плохое расположение в памяти.
 + :
  1) Создает новые деревья, оставляя старые неизменными.
  2) Не надо беспокоиться о том что будут вестись записи в один и тот же участок памяти.
  3) Эффективно комбинировать деревья.

Ассоциативная операция : a+(b+c) = (a+b)+c
Коммутативная операция : a+b=b+a
Не каждая ассоциативная операция является коммутативной и наоборот.
Конкатенация строк или сумма очень большого и маленького числа.

Task - parallel: вычисление задачи параллельно на ядрах компьютера.
Data - parallel: распределение данных на ядра компьютера.
Каждая data - parallel программа мапит входящий элемент к количеству работы нужное на обработку элемента.
Цель data - parallel планировщика: эффективно балансировать рабочую нагрузку между процессорами без знания того, сколько нужно на обработку элемента. Эта обязанность сваливается с плеч программиста.
Параллельный for loop не очень функциональный так как ничего не возвращает, и общаться может только с помощью сайд эффектов, например, назначение элементов массиву.
Не все операции на коллекции можно параллелить, так, например, операция foldLeft нуждается в последовательном выполнении, так как каждый следующий элемент зависит от предыдущего. С другой стороны, fold можно использовать в параллельных операциях. Из этого следует что одно из ограничений на fold это то что его изначальное значение должно быть нейтральным.
Aggregate - собирает в себе логику foldLeft и fold.

Traversable (foreach) => Iterable (iterator) => {Set, Seq [ordered collection], Map} => ...
Такая же иерархия и у параллельных коллекций.
И такая же иерархия у кода-агностика к параллелизму.
Последовательная и параллельная коллекция зависит от Gen... . К примеру, GenSet => {Set, ParSet}.
Внутри там все взаимосвязано.
Такая иерархия позволяет нам использовать параллельный код таким же образом как и последовательный.
Не для всех коллекций есть их параллельные двойники, но они все равно преобразуются в ближайший похожий, так List, преобразуется в ParVector.
!!! При параллельном использовании какого-либо общего участка памяти, обязательно его нужно синхронизировать!!!
!!! Никогда не меняйте параллельную коллекцию на которой происходят операции!!!
!!! Никогда не читайте с коллекции которая параллельно меняется !!!
!!! Никогда не пишите в коллекцию которая параллельно проходится !!!
TrieMap - исключение к последним двум правилам.

Каждая коллекция имеет:
 1) Iterator - trait с двумя методами (next, hasNext). Описывает текущий элемент.
 2) Splitter - trait который расширяет Iterator, и имеет методы (split, remaining). Реализуется в параллельных коллекциях, позволяет разбить коллекцию на части, для параллельной вычисления, так же узнать количество оставшихся элементов.
 3) Builder - trait с методами (+=, result). Описывает создание новой коллекции, при этом, result возвращает коллекцию с добавленными элементами через +=, оставив Builder в неопределённом состоянии.
 4) Combiner - trait который расширяет Builder и имеет метод combine(x: Combine). Вызов этого метода возвращает объединенные коллекции, оставив при этом Combiner'ы в неопределенном состоянии, эффективность комбайна O(logN). 

При реализации метода интерфейса Combiner, нужно внимательно следить за тем, какая коллекция использует этот интерфейс, например, для Set и Map это будет union, для Seq concatenation.
Сделать такую операцию получится максимум с O(m+n).
Hash table - insert, remove and look up затрагивают константное время O(1).
Balanced tree - имеет свойство говорящие о том что его самая длинная ветка не должна быть больше чем в 2 раза самой короткой. Все действия выше занимают O(logN).
Linked list - поиск осуществляется за O(n). O(1) pepped and append, O(1) intersection.
Functional list - O(1) - prepend. O(n) - everything else.
Array List - O(1) - append, O(1) - random access, иначе O(n).

Большинство структур данных могут быть построены как параллельные с помощью так называемого two-phase construction. В данном случае combiner хранит не результирующую структуру данных, а промежуточную.
Такая промежуточная структура данных обладает следующими свойствами:
 1) Имеет эффективный combine метод - O(logN + logM) или эффективнее.
 2) Имеет эффективный += метод.
 3) Может быть конвертирован в результирующую таблицу за O(n/P) времени, где n - размер структуры данных, P - количество процессоров.
Combine операция выполняется за две фазы, сначала у нас собираются все данные добавленные через +=, затем все эти данные приводятся к виду конечной структуры данных.
При выборе структуры промежуточных данных, нужно внимательно следить за конечной структурой данных, и выбрать подходящую. Например для HashTable, такая промежуточная структура данных, которая будет партишонить исходя из вычисления хеш кода.
Search Three, combiner хранит наборы данных в неперекрывающих интервалах, в соответсвии с порядком. И в конце просто соединяет все части в параллели.

List более для последовательных вычислений, в то время как trees для параллельных.
Trees полезны для параллельных вычислений только если они сбалансированы.
Для создания сбалансированного дерево был введен новый тип данных, который называется Conc, в его теле хранится level - длиннейшая часть от корня до листа (высота дерева) и size - количество элементов в под-дереве. 

________________////////////////______________________-------------____________________

В свою очередь, Spark and Scala хороши для огромных вычислений на разных машинах по сравнению с R, Python, MatLab and etc.
У Spark такой же API как и у обычных коллекций у Scala.
Интересно то, что распределенное вычисление в своей абстракции такое же как и параллельное.
С тем исключением, что у нас теперь вместо потоков, задания выполняют разные машины. И теперь нужно учитывать задержку сети.
Так же сохраняются все свойства касаемые того, что операции должны быть ассоциативные.

Две новые проблемы параллельного вычисления:
 1) Partial failure: падение воркера, из-за какого либо исключения или технической ошибки.
 2) Latency: одна операция выполняется значительно дольше других из-за стенного соединения. 
Эти проблемы решаются самим спарком.
Latancy является очень большой проблемой при распределенном вычислении, так что о ней никогда нельзя забывать, так как Spark полностью не решает эту проблему.
При учитывании задержки нужно взять во внимание то, что у нас есть 3 вида переноса данных, которые по своему уровню задержки распределены в следующем порядке:
 1) Memory
 2) Disc
 3) Network

Hadoop обладает особенностью fault-tolerant, это значит что если упадет какой либо worker, у нас не отвалятся все вычисления, а просто будут данные заново пересчитаны.
Преимущество Spark перед MR в том что он оперирует над immutable and in-memory data. Все операции над данными это transformation, как Scala коллекции. Все операции Fault tolerance, так как все трансформации происходят над immutable data collection, то есть мы можем отслеживать путь изменения какой-либо коллекции. Spark дает х100 перфоманс перед Hadoop.

RDD - immutable коллекция данных, очень похожа на Scala List. Использует High-order функции для манипуляции данными. Использовать RDD можно так же как и любые последовательные или параллельные коллекции, только с тем знанием, что RDD - коллекция, распределенная между машинами.
RDD можно получить путем: 1) Из другого RDD. 2) Из SparkContext или SparkSession (путем распараллеливания коллекции и путем чтения из файла).
Есть два главных способа преобразования RDD:
 1) Transformations [Lazy] - создание одного RDD из другого.
 2) Actions [Eager] - Приведение RDD к какому-либо результату (подсчету или записи куда-то).
Lazy/Eager операции уменьшает network communication.
Spark очень умный и делает почти на всем оптимизацию. к примеру если мы имеем action take(10), он не будет считать всю коллекцию, а затем брать 10 элементов, он посчитает только 10 элементов, а замоем вернет их.

Большинство распределенных задач итерративные:
 1) Hadoop: IO Disc -> MR -> IO Disc -> MR -> IO Disc -> ...
 2) Spark: IO Disc -> Work -> Work -> Work ...

Есть замечательная возможность сохранить RDD в памяти машин путем вызова функций cache (сохранить только в памяти) or persist (настроить способ сохранения memory or disc or both, как serialized or not object). Это поможет нам избежать перевычисления. Если данные не помещаются в памяти, то они отбрасываются и перерассчитываются.
Самое главное отличие RDD от других коллекций это в том, что RDD может откладывать вычисление, в отличие от Scala Collection.
Spark хранит цепочку преобразований, которые будут выполняться на данной RDD, и он достаточно умен чтобы применить разные оптимизации, например мы можем написать map(...).filter(...), тут он сделает оптимизацию таким образом, что map и filter отработают за один проход, вместо двойного просмотра коллекции.

Spark имеет одного Master (Driver program) и много Workers, суть их в том, что программа распределяется между workers. Master координирует всю работу, Workers выполняют. Driver и Workers общаются через Cluster Manager.
Spark Application - набор процессов работающих на кластере. Все эти процессы координируются через driver program. На стороне Driver node существует main метод и на его же стороне создается SparkContext, RDD, Actions and etc. На стороне executors происходят вычисления и сохраняются данные.
Spark app initialization: создается SparkConxtext, SparkConxtext коннектится к cluster manager, назначет workers, driver program отправляет код to executors, SparkConxtext отпраляет таски executors.
!!! Foreach метод на RDD выполняется на executor'ах!!!

В Spark нет таких операций как foldLeft/foldRight так как они последовательные.

Одним из основных типов данных в Spark являются key-values pairs. Потому что их удобно использовать в тех случаях, когда данные большие и с ними нужно что-то делать.
В Spark key-value pairs хранятся как Pair RDDs. Так как там представлен широкий набор возможно манипуляции над данными с ключами.
Самые популярные методы:
 1) groupByKey 
 2) reduceByKey - своеобразная сумма groupByKey and reduce. Так же эта операция быстрее из-за того что сначала применяет reduce, затем groupByKey, очень похоже на combiner и reduce в мире MR.
 3) join
PairRDD может быть создан путем применения map на обычный RDD.
 1) mapValues - применяет функцию ко всем значениям в парах.
 2) countByKey [Action] - возвращает Map соответсвующий ключу и количеству элементов соответствующих нему.
 3) keys [Transformation] - получит ключи всех пар.

Есть два вида Join:
 1) Inner join (join). - возвращает скомбинированные значения, которые присутствуют в обоих датасетах.
 2) Outer join (LeftOuterJoin, RightOuterJoin). - возвращает скомбинированные пары, где ключи не должны присутствовать в обоих вхождениях.

Когда мы вызываем groupBy or groupByKey, то Spark под капотом вызывает стадию под названием shuffle, перекидывающая одинаковые ключи на одну машину.
Shuffle - перемешивание данных между машинами.
Определятся какой ключ должен быть передан на какую машину с помощью hash partitions.

Данные в RDD разделены на partitions. Partitions никогда не распространяются между несколькими машинами, я о том, что tuples из одного partition будут  на одной машине. Каждая машина в кластере содержит один или более partition. Количество partitions конфигурируемое, обычно равно количество ядер на всех машинах.
Partition бывает 2-х видов:
 1) Hash partition.
 2) Range partition.
Изменять partition можно только на PairRDD.
HashPartition - (k,v) => (k.hashCode() % partNum). Spark считает у каждого tuple его partition key, и базируясь на нем, отсылает все tuple с таким же partition key на определенные машины. HashPartition распределяет ключи поровну по всем машинам.
RangePartition - распределение данных в соответсвии с порядком ключей. К примеру, распределение меду 5 машинами Int ключей, в равном range по 5. Tuple падает в соответствии с порядком данного ключа и set of sorted ranges of keys. Важно понимать что ключ должен иметь какой-то порядок.
Выставить меру Partition можно с помощью:
 1) PartitionBy - вызвать этот метод на RDD с параметром указывающем partition.
 2) Использовать Transformation с указанным Partition.
Результат Repartition должен быть сохранен в памяти.
Partition на базе трансформации может быть настроен двумя способами:
 1) Наследование от предыдущего partitioned Pair RDD.
 2) Так же применяется partition автоматически тогда когда он нужен по контексту, к примеру на sortByKey or groupByKey, Range/Hash Partition соответсвенно.
Так же есть некие операции которые сохраняют partition, а есть которые не сохраняет, map or flatMap не сохраняют.

Сам по себе Partition нужно знать в тех случаях, когда мы хотим поучить некий перфоманс и уменьшить shuffle.
Shuffle применяется когда result RDD зависит от элементов этой RDD или другой. 
Пример того когда может Repartition помочь.
 1) reduceByKey на pre-partitioned RDD, позволяет сначала посчитать все данные локально, и только затем прогнать результат на Driver.
 2) join на partitioned датасетах, с тем же partitioner, позволяет выполнять join на данных локально на каждой машине.
Обязательно следуя помнить то что есть некоторые операции которые ведут и к сохранению partition и к ее удалению.

Linage graph - вычисления, которые происходят в RDD. Представляются в виде DAG.
Spark может анализировать DAG и выполнять optimizations.
RDD состит из:
 1) Partitions
 2) Dependencies - наследование partitions от его родителя
 3) A function - для получения этого RDD из его родителя.
 4) Metadata - о схеме partition и расположении данных.
Transformations имеют два вида зависимости:
 1) Narrow - каждый partition parent RDD используется максимум одним child partition RDD. Shuffle не нужно, и мы имеем быстрое исполнение операций. (Map, filter, union, join[with partition])
 2) Wide - от одного partition parent RDD могут зависеть несколько child partition RDD. Нужно shuffle, медленное выполнение операции. (groupByKey, join[without partition])
Dependencies - метод, который возвращает последовательность в виде Dependency объектов, говорящих о том как этот RDD зависит от других RDD. Существую следующие типы: 
 Narrow:
  1) OneToOne
  2) Prune
  3) Range
 Wide: 
  1) Shuffle
toDebugString - возвращает linage какого-либо RDD.
Stages - это объединение transformations, которые имеют narrow dependency. 
Благодаря linage of DAG, мы можем говорить о fault tolerance со стороны Spark, так как если отвалится какой либо DAG, мы можем его легко пересчитать, так как у нас есть его метаинформация. Или пересчитать какой-либо отдельный partition.

Передавая структурную информацию в Spark, он делает много оптимизации за вас.
RDD работает над неструктурированными данными, попросту говоря, у данных в RDD нет схемы. Передавая в RDD что-то, Spark знает только о тип данного, но не о его структуре. К примеру у нас не может быть оптимизации связанной с вычислениями, он не может посмотрен внутрь объекта, и черкнуть какие поля надо передавать по сети, а какие нет. С структурой мы можем определиться с тем, какую часть нам надо передавать, и какую надлежащую оптимизацию нужно использовать.
То же самое можно и сказать о вычислениях, в RDD мы не знаем какие вычисления происходят, Spark видит только название функции и не может применять оптимизацию. На структурированных данных Spark может проводить оптимизацию. Например такие как reordering operations

Spark SQL имеет почти все те же оптимизации что и обычные RDBMS.
Spark SQL 3 main goals:
 1) Relational processing - Как и в RDD, так и на external sources.
 2) High performance благодаря техникам, выработанным на RDBMS.
 3) Легкий саппорт новых data sources: слабо и структурированных источников.
Три главных нововведения пришло с Spark SQL:
 1) SQL literal syntax.
 2) DataFrames.
 3) Datasets.
Так же было добавленно два backend компонента:
 1) Catalysis: оптимизатор запросов.
 2) Tungesten: off-heap serialiser (away from the GB).
По идеи SparkSQL построен поверх Spark.
SparkSession является отправной точкой для Spark SQL.

DataFrame равен таблице в RDBMS, концептуально - RDD полный записей и с известной схемой. Так же он не типизированный, Scala не следит за типами во время компиляции и по существу является коллекцией ROW. Transformation на DataFrame называются: untyped transformation.
Создать DF можно с помощью:
 1) From existing RDD - RDD[(T1, T2, ... , TN)] .tDF(*column name*||null), так же если RDD параметризован case class, то на RDD просто можно вызвать toDF.
 2) Считать со специфического data source.
Df.createOrReplaceTempView("*name of view*") - создать временное view для DF, чтобы ссылаться на него из FROM statement.
Spark.sql("*sql query*") - использовать зарегистрированный temp view для совершения запроса на нем.
Sql queries доступных для нас в значительной степени из HiveQL, это включает стандартные SQL запросы.
DF - это:
 1) API над RDD.
 2) Имеет возможность быть агрессивно оптимизированным, за счет того что исследования производимые годами привносят свой вклад в оптимизацию запросов поверх DF.
 3) Untyped.
Для того чтобы было возможно применить оптимизацию, Spark имеет ограниченное количество типов данных (такие же как и в sql). И в дополнение к простым типам данных, так же и сложные, такие как: Map, Array, case class.
StructField - представляет какое-либо поле в диалекте SQL.
Вообще схема в SparkSQL представляется как case class или представляется как Struct... .
Вместе с DF было принесено несколько новых операций, которые уже понимает Spark, и может оптимизировать их для нас, например с помощью filter, он не будет читать все строчки в dataframe. Пример операций: select, where, limit, orderBy, groupBy, join.
Show() - показывает первые 20 элементов dataframes в табличной форме.
printSchema() - возвращает схему DF.
Есть 3 способа сослаться на колонку в DF:
 1) Использовать $: df.filter($"age" > 18)
 2) Ссылаться на df: df.filter(df("age") > 18)
 3) Использовать SQL запрос: df.filter("age > 18")
Filter and where в рамках SparkSQL эквивлентны.
Интересная ситуация складывается с groupBy, где у нас возвращается RelationalGroupedDataset, на котором есть набор агрегирующих операций: count, sum, max, min and avg. Таким образом можно сказать, что после groupBy всегда должна идти какая-либо агрегация.
Агрегация может быть вызвана 2-я способами:
 1) df.groupBy($"attribute1").agg(sum($"attribute2"))
 2) df.groupBy($"attribute1").count($"attribute2")

С unwanted data можно поступать 2-я путями:
 1) drop() - дропает те строчки, в которых все строчки или специфицированные содержат NaN or null.
 2) fill() - заменяет все или специфицированные нежелательные значения на определенный параметр. Replace(Array(...), Map(...)) - заменить колонки в Array содержащие значения в качестве ключей в Map на значения в том же Map.

На DF можно использовать join операции:
 df1.join(df2, $"df1.id" === $"df2.id") [inner join] or df1.join(df2, $"df1.id" === $"df2.id", "right_outer") [right outer join]

Catalysis компилирует Spark SQL программы в RDD. Из-за того что он знает: все типы данных, схему наших структур данных, имеет детальное понимание вычислений, которые мы хотели бы сделать. Это делает для нас возможным делать вычисления с возможностью reordering, fuse, уменьшить количество данных которое мы должны прочитать и не выполнять код на не нужных partitioning.

Tungsten - off-heap data encoder:
 1) highly-specialised data encoders: Так как он знает схему данных он может  хранить данные в более упакованном виде. Это значит больше данных может быть упаковано в память.
 2) column-based format: хранить данные в колонках гораздо выгоднее, так как почти все запросы происходят с ориентированием на колонки.
 3) off-heap: хранит данные вне кучи, и управляет ей сам => не нагружается GB.

DF по факту DataSet[Row].

DataSet:
 1) Может рассматриваться как typed распределенная коллекция данных.
 2) Берет лучшее и устраняет недостатки DF и RDD. Берет методы от обоих.
 3) Требует структурированные и слабо структурированные данные. Schemas and Encoders ядро of DS.
О DS можно думать как о компромиссе между DF and RDD.
C DS мы можем работать так же как мы работали с DF, к примеру мы можем использовать синтаксис как и в DF. DS добавляет множество typed операций, так же мы можем использовать high-order функции: map, flatMap, filter.
Создать DS можно: df.toDS || rdd.toDS || list.toDS, или при считывании файла добавить as[...]. 
!!! DF работает с Column, a DS c TypedColumn !!!
Для создания TypedColumn - $"price".as[Double].
В отличие от DF, где мы имеем только Untyped transformation, в DS мы имеем как typed, так untyped tranformation.
!!! Если на DS мы используем Untyped transformation, то весь наш DataSet потеряет все его typed transformation !!!
Из всего этого следует что на DF так же есть typed transformation, и как результат мы получим DataSet, но вы должны предоставить type information при использовании такого вида операций.
!!! Не все операции что есть на RDD, есть на DataSet. И так же не все операции на DS ведут себя так же как и на RDD !!!
Так же как и на DF, DS после вызова groupByKey, обязательно должна быть вызвана какая-то агрегация. На DS так же можно использовать groupBy, но после агрегаций будет возвращен DF.
MapGroups не знает ничего о partition => вызвает shuffle.

Aggregator[-IN, BUF, OUT] class помогает агрегировать данные. Похож на aggregate функции, 
которые мы видели на RDD.

Encoders - то что конвертирует JVM объект и Spark SQL внутренние представление. У нас генерируется кастомный код для сериализации и десериализации данных. Эти данные хранятся в Tungsten. Он гораздо быстрее чем java сераилизатор, так как знает схему данных и хранит их гораздо компактнее и быстрее обрабатывает. Так же он уже оптимизирован под узкий набор данных для DS.
В классе Encoders уже есть некий набор Encoder для примитивов, String, case class or tule.
Просто нужно вызвать что-то на подобие: Encoders.String || Encoders.product(Person). Такой некий транслятор из row catalyst world в type world.

Есть некие ограничения при работе с Catalyst, например мы можем написать, filter с колонками, там Spark будет знать какие мы колонки и используем и какого типа, с другой стороны при использовании лямбда выражений Spark не имеет представления какие колонки мы используем и вернет весь датасет.
Например операция map на Dataset почти не пользуется работой Catalyst, с другой стороны операция select использует плюсы Catalyst по полной.
Но с другой стороны Tungsten всегда помогает процессу.
Если ваш тип данных не является типом данных из Spark и не является case class, то возможно Tungsten не сможет сохранить ваши данные.



